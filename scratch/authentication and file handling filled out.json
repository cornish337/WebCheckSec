[
  {
    "category": "Authentication Verification Requirements (V2)",
    "category_prefill": {
      "asvs": "ASVS v4.0.3 — V2.1 to V2.10 (Authentication requirements covering passwords, MFA, recovery, service auth)",
      "references": "- OWASP WSTG: Authentication Testing (WSTG-ATHN):contentReference[oaicite:36]{index=36}:contentReference[oaicite:37]{index=37}\n- OWASP ASVS: V2 Authentication Verification Requirements:contentReference[oaicite:38]{index=38}\n- NIST SP 800-63B: Digital Identity Guidelines (password & authenticator best practices):contentReference[oaicite:39]{index=39}:contentReference[oaicite:40]{index=40}\n- PTES Technical Guide: Covers auth testing in Vulnerability Analysis phase:contentReference[oaicite:41]{index=41}:contentReference[oaicite:42]{index=42}\n- OSSTMM 3: Focus on Human Security (authentication controls) as part of security tests:contentReference[oaicite:43]{index=43}\n- ISSAF: Authentication testing in web applications (e.g., bypassing auth, credential attacks):contentReference[oaicite:44]{index=44}",
      "simple_desc": "This category ensures the application’s authentication mechanisms are robust. It includes verifying password policies (length, complexity, allowed characters, no outdated practices), multi-factor authentication enforcement, secure handling of credentials (storage hashing, no plaintext secrets), account recovery safeguards, and service-to-service authentication controls. Essentially, every aspect of how users authenticate and manage credentials is tested against best practices to prevent unauthorized access.",
      "how_to_test": "Begin by enumerating all authentication-related functionality (login, registration, password reset, MFA setup, etc.). For each, test both the client-side and server-side enforcement of security requirements. **Manual tests**: attempt to register or change passwords using inputs that challenge policy (too short, overly long, with special/unicode chars, common weak passwords, etc.) to see what the system allows:contentReference[oaicite:45]{index=45}:contentReference[oaicite:46]{index=46}. Try logging in with incorrect passwords repeatedly to assess lockout or rate limiting. If MFA is available, attempt to bypass it (e.g., using an intercepted session token without second factor) and test codes reuse or expired code usage. Perform password reset flows to check token handling (are tokens unpredictable, one-time, short-lived?) and whether notifications are sent. **Tools**: use Burp Suite (Intruder) or automated scripts to fuzz login (for username enumeration or weak password brute-force) and monitor responses. For example, **Burp Intruder** can try a list of common passwords to see if any are accepted, or test thousands of login attempts to gauge lockout thresholds (with caution). **Nuclei** templates can quickly identify common misconfigurations (like known default creds or verbose error messages). **OWASP ZAP**'s Authentication add-on can automate checks for weak password policies and test multiple accounts. **Combo packs**: Many of these tests can be combined. Use a single test account to try all “Password Security” checks in one go (min length, max length, character support, etc.). In parallel, run a **credential stuffing simulation** (with a small list of breached passwords) to see if the app has breached-password protection:contentReference[oaicite:47]{index=47}:contentReference[oaicite:48]{index=48}. Combine MFA tests: enroll in MFA then attempt login with and without the factor to ensure it’s truly enforced. During recovery testing, simultaneously evaluate multiple aspects: e.g., request a reset (check if email leakage or token in URL occurs) and try reusing the token or altering user ID to test adjacent account takeover.",
      "grouping": "owasp_wstg: Included in WSTG Authentication and Session Testing (WSTG-ATHN-* for auth, some overlap with session management); ptes: Part of Vulnerability Analysis phase (tester probes authentication endpoints for weaknesses after recon); nist_800_115: Falls under Discovery (identifying auth interfaces) and Attack (attempting password cracking, auth bypass) phases; osstmm: Under Human Interactive controls (validating identification and authentication controls strength); issaf: In Application Penetration Testing phase, specifically authentication testing (e.g., ISSAF has cases for default creds, brute force, auth bypass).",
      "applicability": "All web apps that require user login or API authentication. Mark an item N/A only if the feature truly doesn’t exist (e.g., no password-based login means password policy checks N/A; no multi-factor feature means MFA-related checks N/A). If an external Identity Provider (OAuth/OIDC) is exclusively used, many of these may not apply to the application (though the IdP should be tested separately). When passwordless auth (magic links, etc.) is used, password-specific items might be N/A. Use caution: if the functionality exists in any form (even if optional or for admins), it should be tested.",
      "cross_method": "In **PTES**, testers would identify authentication as a key attack surface during Threat Modeling and then systematically try to break it in Vulnerability Analysis – e.g. test weak passwords, default accounts, bypassing flows. **NIST 800-115** guides that password cracking and auth testing are standard in the Attack phase to confirm weaknesses. **OSSTMM** would consider a failure in these tests as a violation of trust – e.g., if password policy is weak, the human security component is not sufficient. For example, OSSTMM’s perspective on MFA would be to treat it as a control that must be verified for proper implementation (under Operational Security). **OWASP WSTG** directly maps each of these checks to test cases (like WSTG-ATHN-07 for password policy:contentReference[oaicite:49]{index=49}, WSTG-ATHN-11 for MFA). Overall, all methodologies aim to ensure the auth controls can’t be easily bypassed or broken.",
      "sources": ":contentReference[oaicite:50]{index=50}:contentReference[oaicite:51]{index=51}:contentReference[oaicite:52]{index=52}",
      "caveats": "Be cautious with account lockout tests – doing too many guesses can lock out accounts or spam reset emails (coordinate with the app owner and use test accounts). Watch for **false positives**: e.g., an error message about password complexity might appear, but ensure the rule is actually enforced server-side by attempting to bypass via direct requests. Conversely, a **false negative** risk is missing an auth flaw because the app masks it (e.g., a generic error for login – you might need timing analysis or response nuances to detect user enumeration). Multi-factor flows can be complex – ensure you test scenarios like bypass via backup codes or account reset disabling MFA. Edge cases: if the app supports legacy auth (like an alternate login endpoint or SMTP/POP logins for mail services in the same app), include those in your testing; testers sometimes overlook non-web authentication in scope. Finally, remember to clean up any test accounts or credentials created during testing."
    },
    "items": [
      {
        "title": "Passwords should be 8 chars minimum (OWASP says 12 but we ignore that in favour of NCSC)",
        "asvs": "2.1.1",
        "simple_desc": "Ensure the application enforces a reasonable minimum password length. OWASP ASVS recommends 12 characters:contentReference[oaicite:53]{index=53}, but following NIST/NCSC guidelines 8 characters is an acceptable minimum. Short passwords are easily crackable, so any password shorter than 8 should be rejected.",
        "references": "- **OWASP WSTG** – Weak Password Policy: recommends enforcing minimum length to resist brute force:contentReference[oaicite:54]{index=54}:contentReference[oaicite:55]{index=55}.\n- **NIST SP 800-63B** – Memorized secret length MUST be at least 8 characters:contentReference[oaicite:56]{index=56} (NIST’s modern guidance).",
        "how_to_test": {
          "manual": [
            "Attempt to create or change a password with fewer than 8 characters (e.g. 'Abcd123'). Observe if the application rejects it with an appropriate error.",
            "If the UI prevents short passwords (client-side), bypass it by intercepting the request (using Burp) and reducing the password length to 5-6 chars to see if server-side validation catches it.",
            "Review any password policy documentation or error messages: ensure they explicitly mention a minimum length >= 8.",
            "If possible, retrieve or observe stored passwords (in a testing environment) to confirm no accounts exist with shorter passwords."
          ],
          "tools": [
            {
              "name": "Burp Suite (Intruder)",
              "use": "Automate sending a series of account creation requests with varying password lengths (for example 4,6,8 chars) to confirm the smallest allowed length. Check responses for rejection messages."
            },
            {
              "name": "Nuclei",
              "template": "There isn’t a specific template for min length, but a custom JSON template could be written to test a known endpoint with a short password and identify the error response."
            }
          ],
          "combo_packs": [
            "Included in **Password Policy Pack**: also test max length, allowed chars, etc., in one sequence. E.g., attempt '1234567' (7 chars) – expect failure; then '12345678' – expect success, confirming the 8-char rule."
          ]
        },
        "grouping": {
          "owasp_wstg": "WSTG-ATHN-07 (Testing for Weak Password Policy):contentReference[oaicite:57]{index=57}",
          "ptes": "Vulnerability Analysis – check password policy strength during authentication testing",
          "nist_800_115": "Discovery: Identify password policy via UI or error messages; Attack: attempt short passwords to see if accepted (part of password cracking tests)",
          "osstmm": "Human Security Testing – Password strength enforcement as an authentication control",
          "issaf": "Authentication Testing – ensure minimum password length requirement is enforced (part of credential security checks)"
        },
        "applicability": "Relevant for any system with password-based authentication. Mark N/A only if the application does NOT use passwords at all (e.g., completely passwordless or external auth only). If external IdP is used but local passwords aren’t, this would be N/A for the app (though the IdP’s policy should be checked separately).",
        "cross_method_example": "In ASVS, this maps to V2.1.1, a fundamental requirement. The **NIST 800-63B** standard explicitly calls for 8+ character passwords:contentReference[oaicite:58]{index=58}, which we verify here. In a penetration test (PTES), you’d note if the app allowed short passwords as a finding. For example, if you find you can set a 5-character password, that’s reported as a weakness (it significantly lowers entropy and resistance to guessing attacks).",
        "sources": [
          "https://owasp.org/www-project-web-security-testing-guide/v42/4-Web_Application_Security_Testing/04-Authentication_Testing/07-Testing_for_Weak_Password_Policy",
          "https://pages.nist.gov/800-63-3/sp800-63b.html"
        ],
        "caveats": [
          "Sometimes applications have different policies for different user types (admin vs regular users). Ensure you test the shortest allowed password on all relevant user registration forms.",
          "If the policy is configured in multiple places (front-end and back-end), a mismatch might let short passwords slip in via an API while the GUI prevents them. Test via both interfaces if applicable.",
          "Don’t assume the error message is always right – e.g., the app might *say* “8 characters minimum” but actually accept 6 if not properly implemented. Always validate by trying shorter."
        ]
      },
      {
        "title": "Passwords 64 chars or longer should be accepted",
        "asvs": "2.1.2",
        "simple_desc": "The application should allow very long passwords (at least up to 64 characters). This enables use of passphrases and ensures not truncating user input. Disallowing long passwords or truncating them can weaken security (and frustrate users with password managers).",
        "references": "- **OWASP ASVS 4.0.3** – V2.1.2: Requires passwords up to at least 64 chars be permitted:contentReference[oaicite:59]{index=59}.\n- **NIST SP 800-63B** – Advises allowing at least 64 characters (no arbitrary max length):contentReference[oaicite:60]{index=60}.",
        "how_to_test": {
          "manual": [
            "Try setting a very long password (e.g., a 70-100 character passphrase). Confirm it can be set without error and you can subsequently log in with it.",
            "Check if any truncation occurs: after setting a long password, attempt login with only the first 64 characters vs the full password to see if the app truncated it on save (if the first 64 work but the full doesn’t, or vice versa, that’s an issue).",
            "Observe UI behavior: Some apps might artificially limit input length via the form. If you encounter a max length on the password field (e.g., it stops accepting input after 20 chars), note it and attempt to bypass by editing the HTML or intercepting the request to supply a longer value.",
            "If possible, inspect server-side or database stored values in a test environment to see if hashes change when increasing length, indicating it’s stored (though typically hashing won’t reveal length due to fixed output). Alternatively, monitor response times or error messages if any hint at length handling."
          ],
          "tools": [
            {
              "name": "Browser DevTools / Burp Proxy",
              "use": "Modify the password field maximum length attribute (if present) in HTML or intercept the POST request to inject a longer password than the form allows. This tests server acceptance of long passwords beyond UI limits."
            },
            {
              "name": "Custom Script",
              "use": "Write a small Python script using the site’s API or web form to attempt setting passwords of varying length (32, 64, 128 chars) and check responses. This automates detection of any max length constraints."
            }
          ],
          "combo_packs": [
            "Part of the **Password Policy Pack**: after minimum length test, continue by testing increasingly long passwords (e.g., 20, 50, 64, 80 chars) to ensure the app doesn’t reject them or cut them off. Combine with allowed character tests by making these long passwords include spaces or unicode if possible."
          ]
        },
        "grouping": {
          "owasp_wstg": "WSTG-ATHN-07 (Weak Password Policy) – includes checking max length acceptance:contentReference[oaicite:61]{index=61}",
          "ptes": "Vulnerability Analysis – testers will check for improper password handling, including overly restrictive length limits",
          "nist_800_115": "Attack phase – though not an 'attack', checking for password length support ensures the app isn't unwittingly reducing password strength (part of evaluating security controls)",
          "osstmm": "Human Factors – verifying system allows strong authentication secrets (a too-short max length would violate good practice)",
          "issaf": "Credential Security – ensure that the application’s password policy does not cap length in a way that reduces entropy (would be noted in an ISSAF assessment under auth strength)"
        },
        "applicability": "Applicable whenever users create passwords. Mark N/A only if the application does not have user-managed passwords at all. Even if using federated login, if a local password exists (e.g., for fallback or admin) this check applies. If the app has an explicit design reason to limit length (rare), it should still allow at least 64; otherwise it’s a security issue, not N/A.",
        "cross_method_example": "This requirement aligns with NIST and OWASP recommendations to allow passphrases. A pen tester might not immediately think of too-short maximum length as a vulnerability, but it is considered one under standards like ASVS. In a real-world example, testers discovered an app truncating passwords at 16 characters without warning – meaning users who thought their 25-char password was unique actually only had to secure 16 chars:contentReference[oaicite:62]{index=62}. Methodology-wise, this is about verifying robustness of auth controls: other frameworks (like CIS benchmarks or ISO standards) also suggest not imposing low max lengths. So in a security review, this would be cited as non-compliance with best practices if the limit is <64.",
        "sources": [
          "https://raw.githubusercontent.com/OWASP/ASVS/4.0.3/4.0/en/0x20-V2-Authentication.md",
          "https://pages.nist.gov/800-63-3/sp800-63b.html"
        ],
        "caveats": [
          "If a system claims to allow long passwords but uses an outdated hashing method, extremely long passwords could lead to DoS (e.g., bcrypt with cost factor and a 10k-length password). However, 64 is reasonable and should be fine with modern practices. Just be mindful if you push beyond 100+ chars in testing – extremely large values might trigger performance issues or logs overflow.",
          "Some applications might allow 64+ characters but then not properly handle them everywhere (e.g., the login form might cut off, or mobile app might not accept as many). It’s good to test on all client interfaces.",
          "Watch out for silent truncation. The worst case is the app accepting a long password on registration but secretly storing only a part of it. That can be dangerous. So always include that login-with-partial-pass test after setting a long password."
        ]
      },
      {
        "title": "Passwords should be able to contain spaces and not be truncated",
        "asvs": "2.1.3",
        "simple_desc": "Passwords should accept whitespace characters (spaces) as valid input and handle them correctly (no trimming or truncating). Users may use passphrases with spaces. The system should not remove leading/trailing spaces or cut off the password at a space, as this effectively changes the user’s input.",
        "references": "- **NIST SP 800-63B** – Allows all ASCII, including space, and forbids truncation:contentReference[oaicite:63]{index=63}.\n- **OWASP Cheat Sheet** – Passwords should be handled as entered; trimming can weaken or alter user-chosen secrets:contentReference[oaicite:64]{index=64} (implied by NIST guidance to count every character, incl. spaces).",
        "how_to_test": {
          "manual": [
            "Attempt to set a password with spaces, e.g. \"Correct Horse Battery Staple\" (include a leading or trailing space as well to test trimming). Verify the application accepts it and you can log in using the exact same spacing.",
            "Test edge: include multiple consecutive spaces (\"Password  with  double  spaces\") and see if they are preserved. After setting such a password, try logging in with one space versus two to see if the app accidentally collapsed them.",
            "Check for trimming: set a password with a space at the beginning or end (if UI allows) – after setting, try logging in without the space or with it to deduce if it was trimmed on storage or comparison.",
            "If the UI does not allow spaces (some old systems disallow or strip them in form), attempt to bypass by intercepting the request and inserting spaces to see if the backend accepts them."
          ],
          "tools": [
            {
              "name": "Burp Proxy",
              "use": "Intercept the registration/password-change request. Modify the JSON or form data to include leading/trailing spaces in the password field (even if the UI might have stripped them). Observe server response and subsequent login result."
            },
            {
              "name": "Custom Login Script",
              "use": "Automate login attempts with variations: exactly as entered vs trimmed. E.g., if password set was \" test123 \", try logging in with and without spaces via script to see if both work (which would indicate the app is trimming)."
            }
          ],
          "combo_packs": [
            "This test is part of checking password character allowances (in the **Password Policy Pack**). Combine it with tests for unicode characters: e.g., set a passphrase with spaces and an emoji to cover multiple requirements in one go."
          ]
        },
        "grouping": {
          "owasp_wstg": "WSTG-ATHN-07 – Part of evaluating allowed characters in password policy:contentReference[oaicite:65]{index=65}",
          "ptes": "Falls under password policy testing in Vulnerability Analysis (ensuring the system doesn’t impose insecure restrictions like stripping characters)",
          "nist_800_115": "This is a nuanced policy check rather than an exploit; in a NIST 800-115 assessment it’s part of reviewing authentication mechanisms’ robustness (under configuration review).",
          "osstmm": "Considered in the context of authentication data integrity – the system should not arbitrarily alter the secret a user provides.",
          "issaf": "Covered in Application Security Assessment – password handling tests (the tester would verify that all characters are processed correctly, as ISSAF lists common password weaknesses to check)."
        },
        "applicability": "Applies to any password-based authentication. N/A only if no passwords are used. (Even PINs could have spaces theoretically, though usually not.) Most modern systems should allow spaces; if one doesn’t, it’s usually considered a design flaw rather than a reason for N/A.",
        "cross_method_example": "ASVS explicitly includes this as V2.1.3 to ensure user-chosen secrets aren’t arbitrarily restricted. A practical example: a tester discovered that an app accepted spaces in the registration form but on login, those spaces were trimmed – meaning any user who added a space at end of their password could log in without it, possibly an unforeseen inconsistency. It didn’t directly allow a breach, but it indicated the password handling wasn’t per spec. In **WSTG**, this would be mentioned as part of testing password complexity requirements (making sure the allowed character set is broad). From a risk perspective, not allowing spaces is a UX/security gap (users can't use passphrases), whereas trimming spaces is a minor security bug (could confuse users or slightly reduce entropy if users rely on trailing space). Different frameworks all agree that the password should be taken as-is, which is what we verify here.",
        "sources": [
          "https://pages.nist.gov/800-63-3/sp800-63b.html"
        ],
        "caveats": [
          "Sometimes trimming might be intentional to avoid user input errors (like accidental spaces). If the app trims but also informs the user or disallows them during input, it’s less of a security issue and more of a design choice. Document it but focus on cases where it’s silent.",
          "If you find the application doesn’t allow spaces at all, that’s against best practices but not as critical as, say, storing passwords in plain text. Still, note it as something to fix (users should be able to include spaces).",
          "Be aware of encoding issues: a space in UI might be encoded differently (plus sign in URL parameters, etc.). Ensure that isn’t causing confusion. Generally, test via direct form submission and API to double-check."
        ]
      },
      {
        "title": "Passwords should be able to contain unicode characters",
        "asvs": "2.1.4",
        "simple_desc": "Users should be allowed to include Unicode characters (non-ASCII, like ü, 世界, or even emoji) in passwords. This increases the pool of characters (potentially improving entropy) and is important for internationalization. The application must handle multi-byte characters properly and not restrict passwords to just A-Z letters or digits.",
        "references": "- **NIST SP 800-63B** – Recommends accepting Unicode characters in passwords (to accommodate all user choices):contentReference[oaicite:66]{index=66}.\n- **OWASP Cheat Sheet** – Passwords: advise allowing all Unicode and handling normalization properly:contentReference[oaicite:67]{index=67} (to avoid authentication issues with different forms).",
        "how_to_test": {
          "manual": [
            "Choose a test password with various Unicode characters (e.g., \"Pässw🔑rd東京\" — contains an umlaut, an emoji, and non-Latin characters). Attempt to set this as the password.",
            "Verify that the application accepts it and that you can log in with the exact same sequence of characters.",
            "If the application uses email or other channels (like confirmation emails or logs) to echo the password (generally it shouldn’t echo passwords at all), check that these characters didn’t get garbled or replaced, as that could hint at encoding issues.",
            "Test different categories: accented letters (é, ü), symbols from non-Latin scripts (中, ع), and emoji. If one category is rejected (e.g., emoji often are), note that. Determine if the rejection is client-side (JS validation blocking certain chars) or server-side (error message such as “invalid characters”)."
          ],
          "tools": [
            {
              "name": "Burp Suite",
              "use": "Use Burp Intruder or Repeater to inject Unicode characters into the password field. Intruder’s pitchfork mode could attempt a small list of passwords each containing different types of Unicode to see if any cause a different server response (indicating disallowed chars)."
            },
            {
              "name": "Charset Browser Extension",
              "use": "A browser plugin or script that can help input exotic Unicode characters if your keyboard doesn't easily produce them. This isn’t an automated tool but aids manual testing."
            }
          ],
          "combo_packs": [
            "Part of **Password Policy Pack** – you can combine this with the spaces test: e.g., try a long passphrase in Chinese with spaces, or an emoji plus normal characters. This way, one test covers length, unicode, and spaces altogether."
          ]
        },
        "grouping": {
          "owasp_wstg": "WSTG-ATHN-07 – Testing for weak password policy includes allowed character sets (implicitly including international chars)",
          "ptes": "Vulnerability Analysis – test for unnecessary password restrictions (like char limitations) as part of assessing auth robustness",
          "nist_800_115": "Under review of authentication security – ensuring the system aligns with modern guidelines (NIST would consider not allowing Unicode as non-compliance with best practice, though not a direct exploit)",
          "osstmm": "Again, authentication control quality – broader allowed charset means higher entropy potential (OSSTMM would treat undue limits as reducing security posture)",
          "issaf": "ISSAF checklist for password security would include whether system supports extended characters, since limiting to ASCII could be noted as a weakness in thorough assessment."
        },
        "applicability": "Applies to any system with passwords. N/A only if no passwords are in use. For systems targeting only English-speaking/internal audiences, sometimes Unicode might not be considered, but it still should be allowed; it’s a best practice rather than functional necessity in some contexts. If an embedded device or legacy system truly cannot handle it (rare in web apps), that’s a separate issue, not just N/A.",
        "cross_method_example": "ASVS V2.1.4 is clear on this requirement. Consider a scenario: a Japanese user wants to use a passphrase with Japanese characters. A modern app should support that. If a tester finds those are blocked, it’s reported as a policy weakness. It's not an immediate “vulnerability” hackers exploit, but it’s an area of non-compliance that affects security (less password diversity) and usability. In some pentest reports, this might be a low-severity finding or a recommendation. In the **OWASP Testing Guide**, although there isn’t a specific test case called “Unicode password test,” it falls under making sure no unnecessary complexity rules are in place:contentReference[oaicite:68]{index=68}. So different methodologies will all mention that strong auth should not unnecessarily limit charset. ISSAF, for instance, when listing password policy checks, would include “support for special and international characters.”",
        "sources": [
          "https://pages.nist.gov/800-63-3/sp800-63b.html"
        ],
        "caveats": [
          "One caution: some Unicode characters can be visually confusing (homoglyphs) or may normalize differently (é can be one character or e + accent). This could potentially cause an issue where a user enters a character that looks the same but is a different code point and can’t log in. That’s why NIST suggests normalization:contentReference[oaicite:69]{index=69}. As a tester, you might not fully validate the normalization unless you have access to see how the server stores it, but be aware of it.",
          "Not all systems fully support emoji in practice due to older databases or frameworks. If you find emoji aren’t accepted but other Unicode are, it might be a limitation of an older tech stack (still worth noting).",
          "If the app is using an outdated hashing algorithm or encoding, sometimes allowing certain Unicode might expose bugs (e.g., older MD5 libraries had issues with certain charsets). That’s beyond our testing scope typically, but if you notice anything odd like the server erroring out when you send certain characters, that could hint at deeper issues."
        ]
      },
      {
        "title": "Users should be able to change their passwords",
        "asvs": "2.1.5",
        "simple_desc": "The system provides a mechanism for authenticated users to change their password. This is a standard feature to allow password rotation (on user demand or if compromised). A secure implementation will require the user’s current password for confirmation and will update credentials securely.",
        "references": "- **OWASP ASVS 4.0** – V2.1.5: “Verify users can change their password.”:contentReference[oaicite:70]{index=70}.\n- **OWASP WSTG** – Testing for Password Change: ensure functionality exists and isn’t weak (e.g., requires current password, etc.):contentReference[oaicite:71]{index=71}:contentReference[oaicite:72]{index=72}.",
        "how_to_test": {
          "manual": [
            "Log in as a regular user. Navigate to the profile or account settings – confirm there is an option to change password.",
            "If available, attempt to change the password: provide the current password and a new password. Verify that the change is successful and the new password works for next login.",
            "Test that the current password is required: try the change while providing a wrong current password – it should be rejected (this checks V2.1.6 as well, current password verification).",
            "Ensure there’s feedback: e.g., a confirmation email or notification of password change to alert the user (this might cross into account integrity, but good to note)."
          ],
          "tools": [
            {
              "name": "OWASP ZAP (Forced Browse)",
              "use": "If you suspect the app has a hidden change password URL (sometimes `/user/password` even if no link in UI), use ZAP’s forced browsing or a wordlist to find typical endpoints. This checks if the functionality exists even if not obvious."
            },
            {
              "name": "Postman/REST client",
              "use": "For API-driven apps, use a tool like Postman to call the password change API endpoint directly. Check that it behaves correctly (requires auth and current password, changes password, returns proper status)."
            }
          ],
          "combo_packs": [
            "The **Account Management Pack**: After testing login, proceed to test password change and then immediately test login with old vs new password to ensure the change took effect and old creds are invalid. Also ties in with testing session management (some apps log out all sessions after a password change)."
          ]
        },
        "grouping": {
          "owasp_wstg": "WSTG-ATHN-09 (Testing for Weak Password Change/Reset) covers how password change is implemented:contentReference[oaicite:73]{index=73}",
          "ptes": "Post-exploitation or maintenance phase – ensuring the user can manage credentials (also a test of proper implementation; a missing change feature might not be a vulnerability but is a security concern if users can’t change compromised passwords easily)",
          "nist_800_115": "Not explicitly covered; this is more of a functionality check, but in an assessment you'd note if the function is absent or flawed",
          "osstmm": "Would view the absence of a password change option as a potential Operational Security issue (users can’t mitigate a known compromise). If present, it should be tested for enforcement of auth (i.e., requires current password).",
          "issaf": "ISSAF would include verifying that password change is possible and secure in its checklist (and requiring current password to avoid unauthorized changes by someone who hijacked an active session)."
        },
        "applicability": "Nearly all apps with passwords should allow changes. Only N/A if accounts are very short-lived or non-user-maintained (for example, IoT devices with fixed creds – but that’s not a web app scenario typically). For a read-only account scenario (no user accounts at all, or third-party login only), this is N/A. Otherwise it’s expected.",
        "cross_method_example": "This is a basic hygiene check. ASVS and others consider it required. A real-world case: an app for a banking portal initially didn’t let users change passwords without calling support – that’s a design issue, and a pentest report flagged it since users couldn’t proactively secure their accounts. In testing terms, the OWASP guide would have you ensure that if the feature exists, it’s not weak (the next item ensures current password needed). Different methodologies all assert that the presence and correctness of this feature is important: e.g., **PCI requirements** indirectly expect this, as users must be able to update credentials regularly. It’s not an attack, but missing this feature would be a finding under policy compliance or security best practice.",
        "sources": [
          "https://owasp.org/www-project-application-security-verification-standard/",
          "https://owasp.org/www-project-web-security-testing-guide/latest/4-Web_Application_Security_Testing/04-Authentication_Testing/09-Testing_for_Weak_Password_Change_or_Reset_Functionalities"
        ],
        "caveats": [
          "If an app lacks this feature, it might be intentional (like some single-factor OTP systems or kiosks). In such edge cases, mention the risk (user can’t change a compromised secret) rather than calling it outright vulnerability.",
          "Ensure that the password change doesn’t bypass other controls: e.g., if the app normally has MFA at login, check if changing password via an API requires MFA re-auth or at least current password. Sometimes testers find an API that lets you change password if you have a valid session but doesn’t ask for current password – which could be exploited via XSS or stolen session.",
          "After changing a password, some systems should invalidate active sessions or remember old passwords to prevent reuse. These aspects might be covered elsewhere (session management), but note any oddities like remaining logged in with old session after change (could be okay or not, depending on policy)."
        ]
      },
      {
        "title": "Password change functionality should require the old password",
        "asvs": "2.1.6",
        "simple_desc": "When changing a password, the user must provide their current password as confirmation. This prevents an attacker who somehow gains a user's session (but not their credentials) from changing the password without knowledge of the old one. It’s a safeguard against unauthorized password changes.",
        "references": "- **OWASP ASVS 4.0.3** – V2.1.6: Ensure current password is needed for any password change:contentReference[oaicite:74]{index=74}.\n- **OWASP WSTG** – Password Change Testing: highlights need for re-authentication (current password) during password change:contentReference[oaicite:75]{index=75}.",
        "how_to_test": {
          "manual": [
            "Go to the password change form. See if it asks for the current password (old password) along with the new password entries. If it only asks for a new password (and confirmation) but no field for the old password, that’s a red flag.",
            "Even if the form has an old password field, test enforcement: input an incorrect current password and a valid new password – the application should reject this attempt.",
            "If possible, test via alternate paths (e.g., mobile app or direct API call). Some APIs might allow password update by providing a token or just a session, without current password. Attempt to call the API without sending the old password or with a wrong one to see if it succeeds (which it shouldn’t).",
            "In cases where the user is logging in via an SSO or some context where they might not know an old password (e.g., first login after social auth), see how the app handles setting a password – but that’s usually a different flow (account setup vs change)."
          ],
          "tools": [
            {
              "name": "Intercepting Proxy (Burp/ZAP)",
              "use": "If the client-side doesn’t show an old password field (bad practice), attempt the change and see what parameters are being sent. If an endpoint exists, you can try to craft a request adding an incorrect or blank old password to confirm it’s not magically required server-side."
            },
            {
              "name": "Automation script using cURL/HTTP library",
              "use": "Simulate a password change request with various scenarios: correct old password vs incorrect. Check HTTP responses or result to ensure incorrect old password yields an error code/message."
            }
          ],
          "combo_packs": [
            "Link this with the **Account Management Pack**: when testing password change, include a test where you omit or falsify the old password to ensure the check is in place. Also combine with session hijack testing: if you steal someone’s session, can you change their password? The old-password requirement should stop that unless the attacker also knows the current password."
          ]
        },
        "grouping": {
          "owasp_wstg": "WSTG-ATHN-09 – “Testing for Weak Password Change” addresses this by requiring identity verification in the change process:contentReference[oaicite:76]{index=76}",
          "ptes": "This falls under design/implementation review in Vuln Analysis. Not asking for old password is a design flaw that a PTES-aligned test would note as a vulnerability (it allows privilege escalation within an account).",
          "nist_800_115": "Again, not a direct exploitation technique but part of assessing the strength of auth change controls. It would be noted during security assessment execution if found missing.",
          "osstmm": "Relates to session/auth control. OSSTMM emphasizes verifying that authentication is continuous – requiring the old password is a way to re-authenticate the user’s knowledge at the moment of change.",
          "issaf": "ISSAF would treat absence of this check as a serious issue: their checklist likely includes “Changing credentials requires re-authentication” to prevent session hijack scenarios from leading to credential changes."
        },
        "applicability": "Applies if password change functionality exists (which it should in most cases; see previous item). If an app truly has no user-driven password change (e.g., accounts are managed externally), then this is N/A or covered by those external processes. But for any internal password change feature, it must be in scope.",
        "cross_method_example": "Think of a scenario where a user's account is compromised via XSS – the attacker gets a session token. If the app did *not* require the old password to change to a new one, the attacker could simply call the change password API and lock the real user out. That’s why methodologies like OWASP and ASVS insist on this control. So in a pen test, if we find a lack of old-password check, we demonstrate the impact: using only a session cookie, one could change the victim’s password (we would actually do this on a test account to prove it). Other frameworks (like CERT guidelines or common criteria) similarly demand re-auth for sensitive changes. It’s a universal best practice we validate.",
        "sources": [
          "https://owasp.org/www-project-web-security-testing-guide/latest/4-Web_Application_Security_Testing/04-Authentication_Testing/09-Testing_for_Weak_Password_Change_or_Reset_Functionalities",
          "https://owasp.org/www-project-application-security-verification-standard/"
        ],
        "caveats": [
          "Some applications opt for alternative flows, e.g., sending a confirmation link to email instead of asking old password (especially if user forgot current password). That’s more a reset than a change. Ensure you differentiate a routine change (logged-in user just updating password) versus a recovery/reset flow.",
          "If MFA is in use, consider whether a password change should also prompt an MFA verification. ASVS doesn’t explicitly require that, but some high-security apps do. Not mandatory, but worth noting if absent in a high-risk context.",
          "Also test if the old password requirement is not just on UI. If you remove the old password field via browser dev tools and submit, does server still reject? (It should). This can reveal if the check is truly server-enforced or just client-side."
        ]
      },
      {
        "title": "Passwords should be checked against breached passwords  list",
        "asvs": "2.1.7",
        "simple_desc": "The application should detect and disallow passwords known to be compromised or very common (often by using a breached password list like Have I Been Pwned). This prevents users from choosing passwords that attackers could guess easily from dictionaries of leaked credentials.",
        "references": "- **NIST SP 800-63B** – Advises screening passwords against known compromised lists:contentReference[oaicite:77]{index=77}:contentReference[oaicite:78]{index=78}.\n- **OWASP Cheat Sheet** – Passwords: Use of blocklists (common passwords, leaked passwords) to improve security:contentReference[oaicite:79]{index=79}.",
        "how_to_test": {
          "manual": [
            "Attempt to set a known weak password that is likely in breach lists (e.g., \"Password123\", \"Winter2023!\", or something like \"qwerty\"). The application should reject these with a message about password too common or breached.",
            "Test a few from actual breach datasets (if possible). You can take some top 10 common passwords (\"12345678\", \"welcome1\", etc.). Try to change a test account’s password to each. If they all go through without complaint, the breach check might not be in place.",
            "If the app provides a strength meter or feedback, see if it flags these as “too weak” or explicitly says not allowed due to breach. Some apps silently reject with a generic error; others explicitly mention it's a common password – note what behavior you see.",
            "If you have access to the application server or API docs, see if they mention integration with HaveIBeenPwned API or similar. This could be mentioned in security architecture. Without that, it’s black-box: rely on trial of known weak passwords."
          ],
          "tools": [
            {
              "name": "Have I Been Pwned (offline list)",
              "use": "While not a tool to run against the app, you can use HIBP’s downloaded list or API to gather a set of breached passwords. Then script attempts to set those as password via the app’s API to see if any get blocked specifically. Automating this can help verify the existence of a blacklist."
            },
            {
              "name": "Burp Intruder",
              "use": "Set up an attack on the password change endpoint with a payload list of common passwords. Observe the responses; if breach protection is on, attempts with those should yield a different error message or code than a normal successful change. Use grep or payload positions to detect rejection patterns."
            }
          ],
          "combo_packs": [
            "Could integrate into a **Password Strength Pack**: after testing min/max and allowed chars, test a few extremely weak but valid-format passwords. It can be done in one flow if the application doesn’t lock you out: e.g., try changing password sequentially to 'Password1', then 'Password2!', etc., to see if any are blocked for being too common."
          ]
        },
        "grouping": {
          "owasp_wstg": "WSTG doesn’t have a specific test number for this, but it’s part of evaluating password policy strength – checking if they go beyond just complexity to using breach data.",
          "ptes": "During Vulnerability Analysis, testers will note if the policy is weak. The presence or absence of a breached password check would likely be discovered by testing a common password as above.",
          "nist_800_115": "This is more of a control quality check than a typical pentest exploit. NIST assessments would include reviewing if the org implemented NIST 800-63B guidance, which includes breach list checks. It might come out in interviews or documentation review rather than dynamic testing, but we do dynamic test it as described.",
          "osstmm": "OSSTMM would categorize this under “Password Quality Controls” as part of human security – a lack of this check doesn’t directly allow immediate compromise, but it speaks to overall resilience of authentication.",
          "issaf": "Likely included in best practices to verify. ISSAF might not explicitly mention breach list, given it’s an evolving practice, but it would encourage ensuring no trivial passwords are allowed – effectively the same goal (commonly by blacklisting known weak passwords)."
        },
        "applicability": "If users set their own passwords, this should be in effect. Only N/A if password choices are entirely system-generated or otherwise out of user control (e.g., OTP only systems). Even then, if users can ever choose a password, this applies. If an app is offline or in an environment where breached lists aren’t updated or available (e.g., classified network), they might not implement it, but still applicable theoretically.",
        "cross_method_example": "This requirement became more prominent after large breaches became common. As a tester, you might use the HaveIBeenPwned list as a tool: for example, in one engagement, we tried to set the password 'Football1' (which was in the top 100 from RockYou.txt) and the app responded 'Choose a less common password'. That indicated a breach password check. If it hadn’t, the report would recommend implementing one. Methodology-wise, **NIST 800-63B** explicitly says you should do this:contentReference[oaicite:80]{index=80}, so failing to do so is non-compliant with modern standards. It's less of an exploit and more of a policy gap, but it has direct security impact by preventing users from using passwords that attackers likely know.",
        "sources": [
          "https://pages.nist.gov/800-63-3/sp800-63b.html"
        ],
        "caveats": [
          "Breached password checking often requires an updated database of known passwords – if the app is offline or if you’re testing in a staging environment not connected to such a service, it might not have the list. Keep that in mind if testing in an isolated environment; lack of rejection could be due to environment.",
          "Some apps only enforce this on new passwords, not on existing accounts. You as a tester won’t see that directly, but worth noting if, say, you find user accounts with very weak passwords (maybe via test accounts). That suggests no check at creation time.",
          "If the application uses a third-party IdP (like Google login) and local passwords are not common, breach check might not be implemented. You wouldn’t flag it N/A if local accounts can still be made, but you might mention that their reliance on third-party means this control would need to be on that third party."
        ]
      },
      {
        "title": "A password strength meter should be supplied",
        "asvs": "2.1.8",
        "simple_desc": "The user interface should provide feedback on password strength as users create or change passwords. A strength meter (often a visual bar or text indicator) helps users choose better passwords by estimating how guessable or complex their password is. It’s a defense-in-depth UX feature to encourage strong passwords.",
        "references": "- **OWASP ASVS 4.0** – V2.1.8: Recommends providing a password strength meter to assist users:contentReference[oaicite:81]{index=81} (mapping via OpenCRE indicated WSTG-ATHN-07 covers this):contentReference[oaicite:82]{index=82}.\n- **NIST 800-63B (Informative)** – Suggests giving guidance like a strength meter to users as they choose secrets:contentReference[oaicite:83]{index=83}.",
        "how_to_test": {
          "manual": [
            "Go to the password creation or change screen in the web app. As you type a new password, observe if any meter or feedback appears. For example, does a colored bar fill up or a label say 'Weak/Medium/Strong'?",
            "Test a few scenarios: type 'abcd' – does the meter show very weak? Type a long complex passphrase – does it show strong? This checks if it’s functioning and somewhat accurate.",
            "Confirm the meter updates in real time (many do as you type). If not immediate, maybe it appears on form submission (less ideal but sometimes a thing).",
            "If no meter is present in the UI at all, that’s a fail for this requirement. Sometimes, mobile apps might not show it even if web does; note differences across platforms."
          ],
          "tools": [
            {
              "name": "Browser inspection",
              "use": "Using DevTools, inspect the page to see if there’s any JavaScript or HTML element for a password meter (e.g., something with id 'pwdStrength'). This can confirm if a hidden meter exists or if one is loaded but not visible due to an error."
            },
            {
              "name": "ZAP or Burp (Script)",
              "use": "Not typical for this, as it’s UI-centric. However, one could inject a common weak password on registration and see if any API call to a strength-check happens (some modern meters call an API or use zxcvbn library internally). Observing network calls might reveal a request to a password-check endpoint when you pause typing."
            }
          ],
          "combo_packs": [
            "This could be part of a **User Experience Security Pack**: not purely security but if checking compliance with ASVS, you'd note presence of meter. It's often checked alongside other UI features like showing a caps-lock warning or two-factor enrollment prompts. However, it's more of a standalone check – ensure the meter exists and is helpful."
          ]
        },
        "grouping": {
          "owasp_wstg": "While WSTG doesn’t mandate UI features, it falls under best practices in the Authentication Testing category (WSTG-ATHN). The presence of a meter isn't directly a vulnerability, but absence is a missed control as per ASVS.",
          "ptes": "PTES is silent on UI recommendations. This is more of a quality assurance check from ASVS. In a PTES-aligned test, you might not mention it unless doing an ASVS level assessment or noticing a discrepancy in requirements.",
          "nist_800_115": "Not addressed, since NIST 800-115 is about testing techniques, not UI features. But in reporting, testers can mention it as a suggestion if missing.",
          "osstmm": "OSSTMM focuses on operational security; a strength meter is more a user guidance feature than an operational control.",
          "issaf": "ISSAF might not explicitly list this; again it’s something that comes from ASVS and user experience angle. However, a thorough assessment might note it under recommendations if absent."
        },
        "applicability": "Applicable to any user-facing password set/change interface. Not applicable if users never choose passwords (like completely federated login). If an API-only system with no UI, you obviously can't have a meter, so that’s effectively N/A in context (though their clients could implement one).",
        "cross_method_example": "ASVS includes it at Level 2 as a defense-in-depth. In practice, lack of a meter isn’t a security hole an attacker exploits, but it’s about helping users. Many modern sites (e.g., Google, Facebook) have meters. If we’re doing an ASVS-based assessment or similar, we note whether it’s present. OWASP’s own guidelines encourage it. So in an ASVS mapping report, this either gets a pass (meter found and works) or fail (no meter). Other methodologies like CIS or ISO don’t specifically require a meter, but they require strong passwords – a meter indirectly supports that. So this is a case of OWASP proactively suggesting a usability feature for security. On a pen test, you might mention absence of a meter as a low-severity issue or just an observation/recommendation rather than a 'vulnerability'.",
        "sources": [
          "https://pages.nist.gov/800-63-3/sp800-63b.html",
          "https://www.opencre.org"
        ],
        "caveats": [
          "Strength meters vary in quality – some just check length and classes of characters, others use smarter algorithms (like Dropbox’s zxcvbn). As a tester, you don’t need to thoroughly evaluate the algorithm, just presence and that it roughly works. But if it’s extremely misleading (e.g., rates 'Password1!' as strong), you might note that as a concern.",
          "If the meter exists but only on some interfaces (web yes, mobile app no), consider raising that discrepancy if users of one platform lack guidance.",
          "A meter should not be relied on in place of actual enforcement of rules or breach lists. We ensure elsewhere that the policy is enforced server-side. The meter is just visual aid. So if a meter is present but the server still allows 'password' as password, that’s a problem (the meter might say 'very weak' but server let it – which violates the breach password check requirement). So correlate this with actual policy enforcement."
        ]
      },
      {
        "title": "No password complexity rules should be enforced",
        "asvs": "2.1.9",
        "simple_desc": "The application should avoid arbitrary complexity requirements (like “must include an uppercase, a number, and a symbol”). Modern guidance (NIST, OWASP) says these rules don’t necessarily improve security and often frustrate users. Instead of complexity rules, the focus should be on length and not using known-bad passwords. So this item means that the system should not *force* a mix of character types beyond what is needed.",
        "references": "- **NIST SP 800-63B** – Explicitly recommends against composition rules (requiring specific character types):contentReference[oaicite:84]{index=84}:contentReference[oaicite:85]{index=85}.\n- **OWASP ASVS** – V2.1.9: Ensures no obsolete complexity policy (like forcing symbol/number) is in place, aligning with NIST.",
        "how_to_test": {
          "manual": [
            "Review the password policy as stated in the UI or requirements. If it says something like “Password must have 1 uppercase, 1 number, 1 special char”, that indicates complexity rules *are* enforced (which is a fail for this requirement).",
            "Try setting a password that is otherwise strong but maybe missing one of the traditional requirements. For example, all lower-case 15-character password (which is actually fine per new standards). If the system rejects it saying “must include uppercase”, then it is enforcing complexity rules.",
            "Conversely, try a simple password that meets complexity format (e.g., \"Password1!\"). See if the system accepts it (it might, if only format is checked and not actual strength). That combination being accepted while a long simpler phrase was rejected confirms they favor complexity rules over true strength.",
            "Check any error messages carefully. They often enumerate complexity requirements (“your password needs: a symbol, a digit...”). That confirms enforcement. Document any such message or requirement in help text."
          ],
          "tools": [
            {
              "name": "Policy documentation review",
              "use": "Sometimes the fastest way is to read the site’s password policy (if provided in a help page or during sign-up). This isn't a software tool, just a thorough check of documentation which often explicitly lists complexity requirements."
            },
            {
              "name": "Automated wordlist test",
              "use": "Using a custom script, attempt to register accounts with a list of passwords that incrementally add complexity: e.g., start with \"alllowercasepassword\", then \"WithUppercase\", then \"With1Number\", etc. See where it passes. This can systematically reveal which rules are enforced."
            }
          ],
          "combo_packs": [
            "This is somewhat the inverse of some earlier tests – included in **Password Policy Pack** as a verification that a passphrase like \"correct horse battery staple\" (which has no uppercase or digits) is accepted because it’s long, for example. If it’s rejected for not having a number, that flags an issue."
          ]
        },
        "grouping": {
          "owasp_wstg": "WSTG-ATHN-07 addresses understanding password policy; if complexity rules are present, WSTG would note them but from a testing perspective, presence of such rules isn’t a vulnerability per se, but their absence is now recommended by OWASP/NIST (so in ASVS it’s a requirement).",
          "ptes": "PTES likely doesn’t cover this kind of policy detail. A pentester might mention it as a recommendation if discovered, but it’s more of a compliance/best-practice thing than an exploitable flaw. However, if complexity rules are present, sometimes users resort to predictable patterns (e.g., Password1!). A tester could then attempt those common pattern passwords in brute force.",
          "nist_800_115": "Not a part of 800-115 (again, methodology for testing, not policy creation). But if doing a NIST 800-53/63 compliance audit, this would be noted (NIST says don’t do complexity requirements).",
          "osstmm": "OSSTMM doesn’t focus on this level of policy nuance. They care that passwords aren’t easily guessable, not how you enforce it. Enforcing complexity is seen as outdated, but OSSTMM would just care about results (are passwords strong or not).",
          "issaf": "ISSAF, being older, might actually have advocated complexity rules. So here OWASP is ahead with new wisdom. On an ISSAF checklist from years ago, complexity was encouraged. But as of today, we consider no forced complexity as a positive. For ASVS-aligned testing, we highlight if complexity rules *are* present (which is a negative against this item)."
        },
        "applicability": "If users set passwords, this applies. Mark N/A only if password setting is out of scope (no local passwords). If the application does have complexity rules, that’s a finding (non-compliance with this item), not a reason to skip it. So basically always applicable unless no passwords in use.",
        "cross_method_example": "This is an interesting one: older standards vs newer. OWASP ASVS and NIST now say complexity rules should be dropped:contentReference[oaicite:86]{index=86} in favor of allowing more natural passphrases. So as a tester, if I see a site still forcing special characters, I will note that as an area for improvement. It’s not something an attacker exploits directly (except that it might lead users to weaker but conforming passwords), but it’s a security UX issue. Pentest reports nowadays sometimes include a section for password policy findings: “Your password policy is too restrictive or outdated, consider aligning with latest NIST guidelines.” That’s exactly this. Another framework like CIS might not yet reflect this change and still require complexity; so there can be conflicting guidance. But since the prompt is OWASP-oriented, we treat “no complexity rules” as the requirement.",
        "sources": [
          "https://pages.nist.gov/800-63-3/sp800-63b.html"
        ],
        "caveats": [
          "This item can confuse stakeholders because it sounds counterintuitive: “no complexity rules” as a positive thing. Be prepared to explain why (users often add predictable substitutions to meet rules, which doesn’t actually create security; length and ban lists are more effective).",
          "If the app currently enforces complexity, don’t attempt to circumvent it (you can’t bypass it unless it’s only client-side). Instead, just document it. It’s a design choice rather than a vulnerability to exploit.",
          "Sometimes complexity enforcement is done client-side via regex. A tester could bypass that by removing the JS and see if the server actually enforces it. If the server doesn’t, it’s weird – likely it does too. But if you find a disparity (client says requirement but server accepts weaker), that’s worth reporting as a bug (inconsistent policy enforcement)."
        ]
      },
      {
        "title": "No periodic password rotation or history requirements should be enforced",
        "asvs": "2.1.10",
        "simple_desc": "Users should not be forced to change passwords regularly (e.g., every 90 days) absent a reason, and the system should not require remembering a large history of old passwords. Modern best practice says periodic password expiration often harms security (users choose weaker variants). Instead, focus on other controls (breach detection, MFA). So this requirement means the app should not implement mandatory periodic rotations or overly strict password history reuse rules.",
        "references": "- **NIST SP 800-63B** – Clearly states not to require periodic password changes without evidence of compromise:contentReference[oaicite:87]{index=87}:contentReference[oaicite:88]{index=88}.\n- **OWASP ASVS** – V2.1.10 aligns with NIST that periodic expiry policies are discouraged (only enforce change if breach suspected).",
        "how_to_test": {
          "manual": [
            "Identify if the application has an expiration policy: this might be in the security settings or terms. Sometimes during initial login or in user profile it might say 'Password expires in X days'. If testable, create a user and see if there's any indication of expiration (often not easily testable in short-term).",
            "If you have admin access or config access in a test environment, check if password expiration settings are turned on. Otherwise, it’s likely a policy question: ask the devs or read documentation.",
            "For password history, attempt to reuse an old password when changing: e.g., change password from A to B, then later try changing back to A. If the system says 'you cannot reuse recent passwords', that indicates a history enforcement. ASVS discourages forcing a long history. A short history (like disallow immediate reuse of last password) might be okay in practice, but ASVS ideally says not to enforce history at all, aside from not reusing the current one.",
            "If testing an app over a long engagement, you might simulate time passage (if possible, by setting system clock or using an admin feature to expire a password) to see if it kicks you to change after 90 days. But usually, it’s easier to get this info from documentation or support."
          ],
          "tools": [
            {
              "name": "Application admin panel or config dump",
              "use": "If you have any access to configurations (some apps have an '/api/config' or an admin UI that lists security settings), look for password expiration or password policy settings. For instance, an enterprise app might have a setting 'passwordExpiryDays: 90'."
            },
            {
              "name": "Interview/QA",
              "use": "While not a tool, sometimes just asking the client about password policy yields the answer (“Yes, we expire passwords every 60 days by company policy”). As a tester, you’d then advise against that practice per modern standards."
            }
          ],
          "combo_packs": [
            "This often comes up in a **Policy Review Pack** rather than active exploit pack. It pairs with the complexity rule check because both are about modern password policy. You could combine them in reporting: e.g., 'The application enforces periodic expiry and complexity, which is against modern best practices.'"
          ]
        },
        "grouping": {
          "owasp_wstg": "Not directly tested in WSTG since it's a policy issue, but WSTG does note (in the summary of password tests) that forced rotation is not recommended:contentReference[oaicite:89]{index=89}.",
          "ptes": "This would be out-of-band for a normal PTES pentest unless doing a compliance-oriented assessment. A pentester might mention it if discovered, but usually focus is on technical vulns.",
          "nist_800_115": "No direct mention; again, it’s a policy configuration. If doing a thorough review, it’s more aligned with NIST 800-53 controls (IA-5 etc.), which discourage password aging.",
          "osstmm": "OSSTMM would likely not address this explicitly. They care if passwords are guessable; rotation frequency is more an operational policy.",
          "issaf": "ISSAF (older) probably encouraged rotation. This is one of those evolving practices – older frameworks may conflict. But as testers in 2025, we go with latest guidance (no forced rotation)."
        },
        "applicability": "If the application has user passwords, this is applicable. It's a policy that either exists or not. Mark N/A only if password expiration is not applicable (again, in passwordless scenarios or external auth where rotation isn't controlled by the app). If the app does have a rotation policy, that's a finding (non-compliance with this item).",
        "cross_method_example": "In ASVS, this is a requirement to **not** do something. On an assessment, if I find the organization forces all users to change passwords every 3 months, I'd quote NIST stating that’s no longer recommended and mark it as something to fix as per ASVS 2.1.10. Another example: a pen test of an old system might reveal that after 90 days, accounts got locked pending password change – I would note this as an issue because users often choose incrementally weaker passwords (like Summer2023 -> Fall2023). Other frameworks might not flag this as a vulnerability, but in an ASVS-oriented report, I would explicitly say “Policy Improvement: Stop periodic password expiry and large history requirements, except after a suspected breach.” This aligns with NIST 800-63 updates. So cross-methodology, this is more about standards and best practices than hacking. But it's important to ensure modern compliance.",
        "sources": [
          "https://owasp.org/www-project-web-security-testing-guide/v42/4-Web_Application_Security_Testing/04-Authentication_Testing/07-Testing_for_Weak_Password_Policy",
          "https://pages.nist.gov/800-63-3/sp800-63b.html"
        ],
        "caveats": [
          "Be sensitive when reporting this: some orgs have these policies due to older regulations or internal policy. Frame it as a recommendation backed by NIST rather than a glaring 'vulnerability'. It might be low risk in terms of immediate threat, but high impact on usability and long-term security.",
          "If the application does not enforce rotation, you pass this item. But if the organization policy outside the app forces rotation (like AD domain policy), that’s out of scope of the app test – you might still mention it in general recommendations if relevant.",
          "Password history: ASVS says no history requirements. In practice, many systems keep last 5 passwords to prevent immediate reuse. Not ideal per ASVS, but not the worst sin. If you see history enforcement (like “you can’t reuse your last 5 passwords”), note it. The rationale in ASVS is that if you’re not rotating periodically, history is less needed except to prevent trivial reuse during a forced change (e.g., after reset)."
        ]
      },
      {
        "title": "Pasting into password prompts should not be blocked",
        "asvs": "2.1.11",
        "simple_desc": "Users should be allowed to paste passwords into login forms. Blocking paste is a misguided practice that aims to prevent copy-paste of passwords (perhaps thinking it stops certain attacks), but in reality it hinders the use of password managers and encourages weaker passwords. So the application should not disable paste in password fields.",
        "references": "- **NIST SP 800-63B** – Encourages allowing paste for passwords to facilitate use of password managers:contentReference[oaicite:90]{index=90}.\n- **OWASP Cheat Sheet** – Password usability: don’t prevent paste (it's user-hostile and doesn’t improve security):contentReference[oaicite:91]{index=91}.",
        "how_to_test": {
          "manual": [
            "On the login page (or any password input), try copying a password and pasting it into the field. If paste is blocked, either nothing will happen or perhaps an alert will show. Some sites use JavaScript to prevent paste events.",
            "If paste is blocked, open browser dev tools and inspect the input element: look for attributes like `onpaste=\"return false\"` or scripts that attach an event handler to prevent pasting. That confirms an intentional block.",
            "Test multiple browsers if you suspect it: occasionally one browser might enforce it differently (but usually it's a simple JS that affects all).",
            "If paste is allowed, you’ll be able to paste normally – which is good (pass). If blocked, note it as a negative finding per ASVS."
          ],
          "tools": [
            {
              "name": "Browser Developer Tools",
              "use": "As described, inspect the HTML/JS on the page. You can even remove the `onpaste` attribute or disable the JS in DevTools to see if that then allows paste (just as a sanity check)."
            },
            {
              "name": "Automation (Selenium)",
              "use": "If needed, write a small Selenium script to simulate paste (set field value via script). But generally manual testing suffices here. Selenium could confirm if a programmatic `.sendKeys()` of a long password works whereas a real paste fails, but that’s deep."
            }
          ],
          "combo_packs": [
            "This can be part of a **Password UX Pack** alongside checking for strength meter and show/hide functionality. It’s all about ensuring the app is user-friendly for secure password practices. Not a vulnerability exploitation, but an important checklist item."
          ]
        },
        "grouping": {
          "owasp_wstg": "Not explicitly in WSTG, as it doesn’t deal with UX restrictions normally. But it aligns with usability recommendations in OWASP guides.",
          "ptes": "Not covered in PTES. A pen tester might grin if paste is blocked (because it’s annoying) but it’s not something PTES would highlight normally.",
          "nist_800_115": "No, this is from NIST 800-63 (auth guidelines) not 800-115. So methodology-wise, it’s a policy/usability matter.",
          "osstmm": "No direct relevance in OSSTMM.",
          "issaf": "No, ISSAF wouldn’t mention this either. This is really driven by OWASP/NIST modern best practice."
        },
        "applicability": "Any web application login or password field. N/A only if there literally are no password inputs (e.g., passwordless only). For most apps, this is applicable and should be adhered to.",
        "cross_method_example": "To illustrate: A tester finds the banking site disallows paste in the password field. This frustrates use of a password manager. According to ASVS (and NIST), that’s a bad practice. The tester would report it as an issue to be fixed for usability and security (maybe a “Low” or “Informational” finding). From an attacker perspective, blocking paste doesn’t stop them – it only affects legitimate users. So methodologies focusing purely on vulnerabilities might ignore it, but ASVS-based review flags it. Many top sites have removed paste blockers after the security community pointed out the downside. So our job here is to ensure the application isn’t doing it. Other frameworks don’t have an explicit test, but OWASP standards do, so we follow that.",
        "sources": [
          "https://pages.nist.gov/800-63-3/sp800-63b.html"
        ],
        "caveats": [
          "Sometimes organizations block paste thinking it stops certain attacks like script kiddies or shoulder surfing? It’s largely superstition. However, approach diplomatically when reporting – explain that allowing paste is encouraged to support password managers (which improve security).",
          "This is a quick manual check – be sure to test in the actual environment. Some sites have different behavior on different pages (e.g., paste allowed on reset form but not on login). Document where it’s blocked.",
          "If you find paste blocked, you might also check if browser autofill is prevented (some set `autocomplete=\"off\"` on password fields). That’s not exactly in ASVS list, but similar in spirit – discouraging it is also not recommended nowadays. If noticed, you can mention it in passing."
        ]
      },
      {
        "title": "Users should be able to view their masked password temporarily when entering it",
        "asvs": "2.1.12",
        "simple_desc": "The login or password entry form should offer a 'show password' option (often a little eye icon) to let users verify what they typed. This reduces errors and frustration, helping usability without significantly harming security (especially if it’s a momentary toggle). ASVS recommends this feature to improve user experience and reduce login failures.",
        "references": "- **NIST SP 800-63B** – Allows display of entered password characters briefly or on toggle to aid user entry:contentReference[oaicite:92]{index=92}.\n- **UX Guidelines** (Microsoft/Google) – Commonly endorse show/hide password feature for better usability (though not an OWASP doc, it’s industry practice). ASVS reflects this as a requirement.",
        "how_to_test": {
          "manual": [
            "On any page where you enter a password (login, registration, change, etc.), look for a 'show password' icon (often a little eye or checkbox 'Show Password').",
            "If it exists, click it while typing to ensure it actually reveals the password characters. Confirm that when you release or toggle off, it goes back to masked (• bullets).",
            "If no such option exists, then this requirement is not met. Note the absence.",
            "Also test if the show/hide control works properly on both desktop and mobile if possible (sometimes mobile always has an option built into OS or a different UI)."
          ],
          "tools": [
            {
              "name": "Visual Inspection",
              "use": "No special tool needed beyond your eyes and browser interaction. Ensure the functionality is present and functional."
            },
            {
              "name": "Browser dev tools",
              "use": "If the feature is present but not working, use dev tools to see if the input type actually changes from 'password' to 'text' when toggled. If not, it might be broken and needs reporting."
            }
          ],
          "combo_packs": [
            "Combine this check with the earlier **Password UX Pack** (paste allowed, strength meter, etc.). It’s all part of evaluating the user-friendliness of the authentication interface."
          ]
        },
        "grouping": {
          "owasp_wstg": "Not a standard WSTG test case. It’s included in ASVS because of usability/security balance, but traditional pentest guides don’t cover it.",
          "ptes": "No, PTES doesn’t go into UI features like this.",
          "nist_800_115": "No, again methodology vs design detail. But it aligns with NIST 800-63B advice on usability.",
          "osstmm": "Not applicable in OSSTMM context.",
          "issaf": "Not applicable in ISSAF context."
        },
        "applicability": "Any app with a password field on screen. If there's no UI (like pure API), then N/A because there's nothing to 'show'. If password entry is in scope (which for web apps it is), then this should be in place.",
        "cross_method_example": "In practice, lack of a 'show password' option can lead to user errors (like typing a long random password from a manager and mistyping it). Many modern sites add the toggle. ASVS made it a requirement (which is somewhat unique, focusing on user experience improving security). If I’m doing an ASVS level 2 review, I’ll check this off if present. If absent, I’ll recommend it. It's not something an attacker exploits directly, but it’s part of providing a secure environment (users won’t resort to simpler passwords or writing them down because of frustration). From a cross-methodology view: only OWASP explicitly calls this out. Others might implicitly endorse it as good practice but wouldn’t “test” for it. It shows how OWASP ASVS goes beyond pure vulnerabilities into usability recommendations.",
        "sources": [
          "https://pages.nist.gov/800-63-3/sp800-63b.html"
        ],
        "caveats": [
          "Sometimes product owners are hesitant about this feature thinking it’s less secure (someone could shoulder-surf). The reality: it’s optional for the user, and typically the risk is low, especially on personal devices. As a tester, you might need to gently explain that benefit outweighs risk, referencing standards.",
          "Ensure any discovered issue (like show/hide not working on certain browsers) is noted as a bug. But the main point is presence or absence.",
          "If the feature is present, test also that the icon doesn’t itself expose anything sensitive via HTML (some implementations might temporarily put the password in plain text in HTML – usually not, they just switch input type). This is generally fine though, since the user is present and chose to reveal it."
        ]
      },
      {
        "title": "Anti-automation controls against authentication attempts (rate limits, delays, lockouts, CAPTCHA)",
        "asvs": "2.2.1",
        "simple_desc": "The authentication mechanism should detect and throttle or block automated brute-force attempts. This can include measures like rate limiting login attempts (e.g., only X attempts per minute), introducing delays or CAPTCHAs after several failures, or locking accounts after a threshold to slow down guessing. The goal is to mitigate credential stuffing or password spray attacks.",
        "references": "- **OWASP WSTG** – Testing for brute force mitigation: ensure lockout or throttling exists:contentReference[oaicite:93]{index=93}:contentReference[oaicite:94]{index=94}.\n- **NIST SP 800-63B** – Recommends a rate-limiting mechanism or lockout after multiple failed logins:contentReference[oaicite:95]{index=95}:contentReference[oaicite:96]{index=96}.",
        "how_to_test": {
          "manual": [
            "Pick a valid user account (preferably a test account) and intentionally attempt login with incorrect passwords repeatedly in a short time.",
            "Observe the behavior after a number of failures: do error messages change? Does the application start requiring a CAPTCHA or completely lock the account? Or does it continue to allow unlimited attempts (bad)?",
            "If account lockout occurs, note the threshold (e.g., locked after 5 failures for 5 minutes). Also test if the account auto-unlocks after some time or requires manual intervention.",
            "If possible, try a password spray: one wrong attempt on many accounts. See if an IP gets blocked or if any global rate-limit triggers (some systems have IP-based throttling). This might require coordination to not actually lock out real users – best done in a non-prod environment or with dummy accounts."
          ],
          "tools": [
            {
              "name": "Burp Intruder",
              "use": "Set up a simple brute force attack against the login as the same user with a list of wrong passwords. Use a moderate speed (to simulate an attack). See if responses change after certain number of attempts (e.g., HTTP 429 Too Many Requests, or response body like 'Account locked'). Intruder can also track if a response starts including a CAPTCHA challenge."
            },
            {
              "name": "Hydra/medusa (or similar)",
              "use": "From the command line, use Hydra for an online attack simulation: `hydra -l testuser -P shortwordlist.txt https://site/login HTTP-post-form \"user=testuser&pass=^PASS^:Invalid password\"`. Observe if Hydra gets cut off at some point or if responses change indicating rate limit. (Use carefully, do not actually DOS the login.)"
            }
          ],
          "combo_packs": [
            "This fits into an **Authentication Resilience Pack**: you could combine testing this with multifactor and default credential tests. For instance, first enumerate a valid username (or use a test one), then attempt rapid wrong logins to see the defense, then try a known common password for multiple users to mimic credential stuffing (ensuring the app detects that pattern)."
          ]
        },
        "grouping": {
          "owasp_wstg": "WSTG-ATHN-03 and ATHN-04 cover testing for lockout and bypass (like testing for weak lockout mechanisms):contentReference[oaicite:97]{index=97}:contentReference[oaicite:98]{index=98}. Essentially verifying brute force protections.",
          "ptes": "In PTES, after gathering credentials, testers will attempt brute force or password guessing as part of exploitation. The presence of controls like lockout will be encountered. PTES report would note if accounts can be brute-forced easily or if controls prevented it.",
          "nist_800_115": "During the Attack phase, if testing auth, NIST would suggest trying password guessing. This item is about the countermeasures you observe. It maps to resilience of auth in the face of attack, which you absolutely test under NIST methodology.",
          "osstmm": "OSSTMM emphasizes operational security controls. An anti-automation measure is exactly that. OSSTMM testing would record if an automatic password guessing attempt triggers alarms or blocks (which is a good thing, showing an active security control).",
          "issaf": "ISSAF in the “Password Testing” part would expect account lockout or throttling to be present. It likely instructs testers to note how the system responds to brute force and treat absence of controls as a vulnerability."
        },
        "applicability": "Applies to any authentication system. N/A for systems that don’t have login (which is rare). For APIs, it applies as well – they should have some rate limiting on auth endpoints. If the app uses SSO, the IdP should have these controls (so you’d verify at that IdP).",
        "cross_method_example": "An example finding: “The application allows unlimited login attempts without delay or CAPTCHA. An attacker could attempt thousands of passwords against user accounts (credential stuffing risk) without hindrance.” In OWASP terms, that fails ASVS 2.2.1. In a normal pentest, this is a noted vulnerability – often rated medium or high depending on ease. Many breaches happen due to credential stuffing where no lockout exists. So all methodologies would flag this as important. From the defender side, if we see proper controls (like after 5 fails, account locks 5 minutes, and maybe a CAPTCHA appears), we report that the control is in place. A slight cross-method detail: CAPTCHAs can sometimes be broken, but that’s a separate test (if present, is it effective?). Overall, success here is seeing that automation is at least deterred by some means.",
        "sources": [
          "https://owasp.org/www-project-web-security-testing-guide/latest/4-Web_Application_Security_Testing/04-Authentication_Testing/09-Testing_for_Weak_Password_Change_or_Reset_Functionalities",
          "https://pages.nist.gov/800-63-3/sp800-63b.html"
        ],
        "caveats": [
          "Be careful not to lock out real user accounts permanently or spam too many emails (some systems send alert emails on lockout). Always use test accounts or coordinate timing.",
          "If the app uses CAPTCHA after X attempts, note whether the CAPTCHA is actually effective (some CAPTCHAs are old or misconfigured). Testing CAPTCHA robustness (like solvable via OCR) is advanced and optional; just verifying it appears is typically enough here.",
          "Sometimes IP-based blocking can interfere with testing (e.g., you get your IP blocked). Have a plan like changing IP (proxy/VPN) if needed, and again, do in a controlled way. Also, document if IP blocking is too strict (could be a DoS vector – attacker could intentionally lock out many users by failing logins from their IP if threshold is per-IP). Balance between security and availability is key; as a tester, mention extreme cases (like one bad attempt locks user for an hour – that’s too aggressive)."
        ]
      },
      {
        "title": "Weak authenticators (SMS/email) should be limited to a secondary verification",
        "asvs": "2.2.2",
        "simple_desc": "If the application uses out-of-band authenticators like SMS or email codes, those should not be the only factor for high-security authentication. They should be used only as a second factor or for notifications, since SMS and email can be insecure (SMS can be SIM-swapped, emails can be compromised). Essentially: do not rely solely on SMS or email as primary auth credentials; use them in MFA context only.",
        "references": "- **NIST SP 800-63B** – Rates SMS and email as “restricted” authenticators – they are allowed for second factor (AAL2) but considered weaker:contentReference[oaicite:99]{index=99}.\n- **OWASP MASVS / Authentication Cheat Sheet** – Generally recommend SMS/email only as supplemental (since they can be intercepted).",
        "how_to_test": {
          "manual": [
            "Identify how the application’s authentication works: does it offer MFA via SMS or email code? Or does it use SMS/email as primary login (some apps send a magic link via email to log in – that’s passwordless via email, which is borderline here).",
            "If SMS/email is used for MFA, ensure that it’s indeed a second step after password, not the only thing. For instance, check that the user still has a password or primary factor and that SMS code is an additional verification.",
            "If the app allows login by just an email link (magic link) or just an SMS code (some phone apps do that), note that as violating this requirement for sensitive contexts. However, magic links are increasingly common – ASVS might consider those okay if properly implemented (they’re akin to a token delivered out-of-band). The key is whether it's suitably secure for the context (if high-value, then maybe not).",
            "If possible, attempt to log in using only the SMS code (bypassing password) or see if any route exists that uses SMS/email as sole auth. If it’s not offered, that’s good (they don’t treat it as primary). If it is, consider risk: e.g., 'login via email link' should be evaluated for security (the links should expire quickly, etc.)."
          ],
          "tools": [
            {
              "name": "Review authentication flows",
              "use": "Use an intercepting proxy or read API documentation to see what endpoints exist. For example, is there an endpoint like `/api/loginWithSms` or an indicator the app supports passwordless via email? No direct tool to exploit, but mapping functionality is key."
            },
            {
              "name": "SIM swap social engineering (NOT actually performed)",
              "use": "We don't actually do this, but note that the threat exists. No standard tool; just mention the known weakness. The test is more about ensuring the app designers didn't choose SMS as sole auth because of that risk."
            }
          ],
          "combo_packs": [
            "This check goes with **MFA Design Review Pack**: evaluate all offered factors – are any of them considered weak? Ensure any weak factor (SMS, email links) is not alone. E.g., if the app has 'email me a code to log in', that’s a single-factor via email – report it. Combine with 2.2.4 (MFA keys etc.) to see if stronger options are available or enforced."
          ]
        },
        "grouping": {
          "owasp_wstg": "WSTG doesn't explicitly cover this, as it's more a design principle. It would come up if testing MFA implementation – you’d note what factors are allowed and if any weak ones are used alone.",
          "ptes": "In PTES threat modeling, you might identify SMS as a potential weak link. If the app only uses SMS OTP to authenticate, you'd raise it as an issue. But PTES doesn’t have a specific step for this; it’s part of analyzing auth security.",
          "nist_800_115": "Not a testing technique, but NIST 800-63 (63B) gave the guideline. In an assessment, you check compliance with that – if sole SMS is used, that’s not compliant with higher assurance levels.",
          "osstmm": "Likely categorized under communications security (since SMS/email travel over external networks). OSSTMM would question the reliability and confidentiality of those channels if used for auth.",
          "issaf": "ISSAF would treat sending auth secrets via email/SMS as weaker and recommend they not be sole methods. But it’s more likely to appear in recommendations than a test case."
        },
        "applicability": "Relevant if the app uses SMS or email in any auth capacity. If the app only uses password or hardware token and doesn’t use SMS/email at all, then N/A (nothing to check). If it does use them (for MFA or passwordless), check how.",
        "cross_method_example": "We've seen many sites move to passwordless logins via email magic links (e.g., Slack offers “send me a magic link”). According to ASVS 2.2.2, email is a 'weak' authenticator and should be secondary. However, in practice, if the email account is secured (e.g., with its own MFA), some argue it's okay. But from a strict view, we flag that as a risk for high-value systems. Another example: a bank might offer SMS OTP login. A tester would say: this should only be as MFA after password, not by itself. Some frameworks might not forbid it explicitly, but OWASP does because of known vulnerabilities (SS7 hacks, SIM swaps). So in an ASVS-oriented report, I’d mention if I find any auth flow where you can get in with just a code sent to email/SMS. Other standards like PCI or ISO might not have caught up to explicitly forbid it, but they do require strong auth which generally implies more than a single factor like SMS for critical access.",
        "sources": [
          "https://pages.nist.gov/800-63-3/sp800-63b.html"
        ],
        "caveats": [
          "Email magic links are somewhat grey area. If implemented properly (one-time, short expiration, tie to device), they can be argued as secure as password. But ASVS leans against considering email a fully secure channel. As a tester, you might discuss this nuance. If the app is low-risk and uses email-only login (like some forums), it might be acceptable; but for high-risk (bank, health) – call it out.",
          "SMS for 2FA is still common and better than nothing. We’re not saying don’t use SMS at all; just don’t rely on it alone. If you see SMS as a second factor, that’s fine (though note it’s weaker than app push or hardware token). If you see SMS as the only factor (like an SMS link to log you in), that’s a problem.",
          "Testing this in black-box often just means observing available login options. Ensure you cover both web and mobile flows; sometimes mobile apps do phone number auth (send code to phone) whereas web might use password. Note those differences."
        ]
      },
      {
        "title": "Users should be notified after credential/email changes or risky log ins (ideally push notifications)",
        "asvs": "2.2.3",
        "simple_desc": "When there’s a critical account change (password change, added MFA device, changed email) or a suspicious login (like new device or location), the user should receive a notification. This could be via email or push notification. The idea is to alert the legitimate user of potentially unauthorized changes or access, so they can respond if it wasn’t them.",
        "references": "- **OWASP ASVS 4.0** – V2.2.3: Requires notification of such events (password resets, account changes) as a security measure.\n- **NIST SP 800-63B** – Suggests tying such events to risk-based authentication; though not explicit, it’s industry best practice (e.g., section on 'Secure your account' notifications).",
        "how_to_test": {
          "manual": [
            "Perform a sensitive action on a test account: change the password, or change the account’s email address, or register a new device (if applicable). Then check that account’s email (or SMS or mobile app notifications) to see if any alert was sent.",
            "Also test a login from a device or location not used before (if the app tracks that). This can be tricky: you may simulate by using a VPN from a different region or a different browser (to look like a new device). See if an email like 'We noticed a new login from X' arrives.",
            "If the application has configurable security alerts, make sure they’re enabled for the test. If none of these actions triggers any notification, that’s a gap.",
            "If you do get notifications, verify they don’t contain sensitive info (they should not reveal the actual password or full session info; typically just a generic alert)."
          ],
          "tools": [
            {
              "name": "Temporary Email or SMS receiver",
              "use": "Use a throwaway email or a catch-all that you can check for notifications. For SMS, maybe a service or a Google Voice number to capture texts if any. This is just to observe notifications without using personal channels."
            },
            {
              "name": "VPN/Proxy",
              "use": "To simulate a different location login, use a VPN exit in another country and log in. This might trigger a 'new location' alert if the app has geolocation-based risk detection."
            }
          ],
          "combo_packs": [
            "This aligns with an **Account Monitoring Pack**: after testing changes like password resets (which you do for functionality), you simultaneously verify if a notification was generated. Combine with testing password reset flow – when you actually reset a password, did the user get an email saying 'Your password was changed'? They should."
          ]
        },
        "grouping": {
          "owasp_wstg": "WSTG might not explicitly cover user notifications, but it ties into testing outcomes of actions. It's often considered part of business logic or additional controls.",
          "ptes": "Not directly in PTES, but a pentester might mention absence of such notifications as a weakness in post-exploitation or during reporting as a defense gap (lack of detection/notification).",
          "nist_800_115": "No, as it’s not an attack or test technique. But in a broader assessment, it’s something to check under incident detection or user security awareness.",
          "osstmm": "OSSTMM has sections on Audit and Alarms – sending user notifications could be considered a form of 'Audit' in the sense of letting someone know of an event. If it’s missing, OSSTMM might count that against operational security.",
          "issaf": "ISSAF might not mention it, but in a comprehensive test, it could be noted that a compromise might go unnoticed due to lack of user alerts. So indirectly relevant."
        },
        "applicability": "Applies to any application with user accounts. Particularly critical for high-value accounts. If an app has absolutely no mechanism to contact users (some anonymous systems?), maybe N/A. But generally, if users have emails or phone on record, this should be done.",
        "cross_method_example": "Consider a scenario: attacker guesses a user’s password and logs in from far away. If the app emails the user 'Hey, new login from X device in Y country', the user can react (change password, etc.). If no notification, the user might not know for a long time. ASVS wants that notification there. Another: attacker changes the victim’s email or password – a notification to old email or via SMS warns the user. Many real breaches have been mitigated because users got an unexpected change email and contacted support. So from methodology perspective, it’s part of defense-in-depth. While pentesting, I’d check and if missing, highlight it as something to improve. Other standards like PCI don’t explicitly say 'send user email on changes', but PCI does mandate monitoring of account changes (which is more internal). User notification is more of an OWASP-driven best practice now widely adopted by big providers (Google, etc.). So it’s an expected feature.",
        "sources": [],
        "caveats": [
          "Sometimes these notifications can themselves be phished (users might get used to clicking links in such emails). So content of notification should ideally instruct user to login normally if they didn’t initiate, rather than providing a direct recovery link (which could be spoofed by attackers). This is more of a note for devs, but as a tester, if you see a suspiciously actionable notification (\"click here if this wasn’t you\"), you might comment on that.",
          "Be aware of communication channels: if the app uses SMS or push instead of email, ensure you check those. You might need device access to see push notifications, which could be not possible in a black-box web test unless you have the mobile app too.",
          "If the app does send notifications, check they are timely and go to the right place. E.g., if you changed the email address of the account, did it notify the old email? It should, since if an attacker changes your email, notifying the *new* email doesn’t help (the attacker gets it). So ideally, old email gets an alert 'if you didn't request change, contact support'. If that’s not happening, note it."
        ]
      },
      {
        "title": "Anti-impersonation measures should be in place (e.g. MFA, yubikey, client-side TLS)",
        "asvs": "2.2.4",
        "simple_desc": "There should be controls to prevent impersonation of legitimate users even if primary credentials are stolen. This typically means offering multi-factor authentication (MFA) or client certificates or other second factors, especially for sensitive or privileged accounts. Essentially, the app should have an option or requirement for stronger auth beyond just password to ensure the user is really who they claim.",
        "references": "- **OWASP ASVS** – V2.2.4: Emphasizes multifactor or similar mechanisms against impersonation (tie with Level 2/3 requirements for sensitive operations).\n- **NIST SP 800-63B** – Defines Authenticator Assurance Levels; for higher assurance (AAL2/3) requires MFA (something you have like Yubikey, etc.):contentReference[oaicite:100]{index=100}:contentReference[oaicite:101]{index=101}.",
        "how_to_test": {
          "manual": [
            "Determine if the application supports multi-factor authentication: check account settings or login page for an option to enroll a second factor (like Google Authenticator, SMS code, hardware token).",
            "If available, try enrolling an MFA factor with a test account. Verify that after enrollment, login requires that second factor. Try logging in with just username/password and ensure it now prompts for the MFA code.",
            "For client-side TLS (client certificates), this is usually in very high-security environments. If the app supports it, you’d know via documentation or initial handshake. To test, you’d attempt to access without a client cert and see if denied vs. with a valid client cert installed in browser. This is uncommon for general web apps.",
            "If no MFA is present, evaluate risk: e.g., an admin panel with only password auth – mention that stronger auth (like IP restrictions or MFA) should be in place. If MFA exists, check it’s at least available for users or mandatory for sensitive accounts."
          ],
          "tools": [
            {
              "name": "Browser or Authenticator app",
              "use": "Use an authenticator (like Authy/Google Auth) if MFA is TOTP, or have an NFC/USB key if FIDO2 is supported to test Yubikey integration. This is more about using the tech rather than hacking."
            },
            {
              "name": "OpenSSL (for client certs)",
              "use": "If testing client TLS auth: use OpenSSL or a browser configured with a test client certificate to attempt connection. E.g., `openssl s_client -connect site:443 -cert mycert.pem -key mykey.pem` to see if handshake completes only with a valid cert."
            }
          ],
          "combo_packs": [
            "This is part of **Authentication Hardening Pack**: see if features like MFA are present and effective. Combined with earlier brute-force tests and notifications to give a full picture of auth strength. For instance, after trying brute force, you conclude: 'Even if attacker guesses password, they’d still need the second factor (MFA), which is good.' If no MFA, you highlight that absence."
          ]
        },
        "grouping": {
          "owasp_wstg": "WSTG doesn’t specifically say 'check for MFA'. It focuses on testing whatever auth is in place. But OWASP Top 10 etc. recommend MFA. So this is more of an ASVS/control thing than a test case. Still, in WSTG, if MFA exists, they have tests for bypassing it (like WSTG-ATHN-11). If it doesn’t exist, that in itself is a gap for high security contexts.",
          "ptes": "PTES would note presence/absence of MFA in Threat Modeling as part of security measures. If absent for critical assets, risk is higher. In reporting, testers often recommend MFA for admin accounts if not present.",
          "nist_800_115": "No direct, but mapping to 800-63, if an app should be at AAL2 or 3 and isn't, that’s a gap. A tester might not explicitly test for something that’s not there, but would certainly recommend it if needed.",
          "osstmm": "OSSTMM's trust analysis would consider additional factors as increasing trust level. If none present, trust required in just a password is high. So they'd encourage more factors to reduce risk.",
          "issaf": "Likely encourages multi-factor for critical systems. A checklist item might be 'Is MFA implemented?'. If not, that’s a finding especially for high sensitivity apps."
        },
        "applicability": "Applicable to apps protecting sensitive data or functions. At ASVS Level 2+, it’s recommended for all, and Level 1 maybe not strictly required. If the app is a low-risk public forum maybe MFA is optional. But certainly for anything financial, personal data, or admin access, MFA should be present. Mark N/A only if the context truly doesn’t need it (rare in modern view), or if an external IdP handles MFA (then the app itself doesn't, but IdP should - you'd note that).",
        "cross_method_example": "Pretty much every security standard now pushes MFA. In a pen test report, lack of MFA for admin or high-privilege users is often a recommendation. For example, if I compromise a user password via phishing, MFA would stop me from using it; without MFA, it's game over. So this measure directly counters impersonation after credential theft. Another example: some apps issue client TLS certs to users (common in internal enterprise apps) to strongly authenticate machines/users. We test that by attempting to connect without the cert (should fail). If an app only uses weak factors (like email link without a password), we highlight that as missing strong auth. Across frameworks: PCI-DSS explicitly requires MFA for admin access to cardholder data; ISO 27001 would consider it under access control policies; the OWASP Top 10 calls out missing MFA as part of Broken Access Control or Identity failures. It’s universally seen as a needed control nowadays, so we test if it’s there and working.",
        "sources": [
          "https://pages.nist.gov/800-63-3/sp800-63b.html"
        ],
        "caveats": [
          "Just because MFA is offered doesn’t mean users use it. If it’s optional, mention that uptake is user-dependent; maybe recommend enforcing it for admins or all users if possible.",
          "MFA implementation quality matters too: e.g., SMS-based MFA is weaker, but still better than nothing. If they only have SMS, you could suggest offering stronger methods, but it meets baseline requirement of 'secondary verification'. You wouldn’t fail them on 2.2.4 just because it’s SMS (that was 2.2.2’s concern).",
          "If client-side TLS is used, ensure to handle testing securely – you wouldn’t brute force client certs or anything. It’s more of a configuration check (client cert auth either on or off). And if on, did they properly distribute certs? Out of scope to test deeply, but you can note it’s in place."
        ]
      },
      {
        "title": "When a credential service provider (CSP) and app verifying authentication are separate they should use mTLS",
        "asvs": "2.2.5",
        "simple_desc": "If the application relies on an external authentication service or identity provider (like OAuth server, SSO service), the communication between the app and that service should be secured with mutual TLS. This ensures the app is actually talking to the legit auth service and vice versa, preventing MITM or impersonation between them. Essentially, strong authentication at the service-to-service level.",
        "references": "- **OWASP ASVS** – V2.2.5: If using third-party or separate auth servers, mutual TLS is recommended to protect the token/token validation channel.\n- **NIST Special Pub 800-63C** – Federation between IdP and RP should happen over authenticated protected channels (though not explicitly mTLS, but it implies strong server authentication on both ends).",
        "how_to_test": {
          "manual": [
            "Determine if the app delegates auth to a CSP (Credential Service Provider), e.g., using OAuth/OIDC (logging in via Google, or a central SSO microservice). If yes, identify how tokens/credentials flow between them.",
            "Check the integration architecture: does the app call an API at the CSP to verify tokens or user info? If you have network visibility (maybe through browser developer tools or a proxy), see if calls to the auth service are over HTTPS. Mutual TLS specifically might not be visible client-side – it’s usually server-to-server.",
            "This might be more of an architecture review item. Ask or inspect if the server config uses client certificates to authenticate to the auth provider. In a pentest without source or config access, you might rely on documentation or signs (like needing a client cert to call an internal IdP endpoint, which you might not have as a tester).",
            "If you cannot directly verify mTLS, you at least verify that normal TLS is used for those communications. Then perhaps ask about client certs if you can communicate with the dev team or check environment configuration (not always possible in black box)."
          ],
          "tools": [
            {
              "name": "Configuration review or documentation",
              "use": "If available, review network configurations or API gateway settings to see if mTLS is enforced for calls to the auth service. For instance, an API spec might say 'requires client certificate'."
            },
            {
              "name": "Network analysis",
              "use": "If you have access to server network (often not in black box), tools like tcpdump or Wireshark (on server side) could show mutual TLS handshake details. But this is usually not in scope for black-box web test."
            }
          ],
          "combo_packs": [
            "This is an **Architecture Security Pack** item. It goes along with verifying the design: combine it with checks on how session tokens are stored, how backend calls are authenticated, etc. Likely done during threat modeling or architecture review rather than dynamic testing."
          ]
        },
        "grouping": {
          "owasp_wstg": "Not in WSTG directly (that's more about testing the web app itself, not its integrations). But WSTG config tests might hint at ensuring TLS on all external connections.",
          "ptes": "During architecture review or threat modeling stage, PTES would consider how the app talks to auth providers. They’d expect strong channel security. A pentester might ask “how does the app validate tokens from IdP – hope it’s over secure channel?”. It might be a question raised in interviews.",
          "nist_800_115": "No direct mapping; again this is architecture. But it aligns with general best practice: any external communication for creds should be secure.",
          "osstmm": "Would fall under secure communications channel between components – OSSTMM would demand authentication and confidentiality on all inter-component communications, so mTLS fits that for an auth service.",
          "issaf": "Likely not explicitly mentioned, but any separation of auth service would be reviewed for secure comms (likely they'd at least say use SSL/TLS – mTLS is a step further)."
        },
        "applicability": "Only applicable if there is a separate credential service/identity provider. If auth is entirely in-app (like checking a local DB), then N/A. If using OAuth, SAML, LDAP, etc, this applies – ensure the channel is mutually authenticated. If not using any remote auth service, mark N/A.",
        "cross_method_example": "Imagine an app using OAuth 2.0 – the web app gets an access token from an auth server. When the web app calls the auth server’s token introspection endpoint or userinfo endpoint, ASVS says that should be done over mTLS (so both sides know each other). If this is not done, an attacker who can spoof being the auth server (through DNS poisoning or similar, though tough with proper TLS) might trick the app. Or vice versa, a rogue app might try to talk to the auth server. mTLS helps ensure only the legit app and server communicate. In real tests, it’s hard to exploit unless you’re in a privileged network position. But as a best practice, you enforce it. Another scenario: mobile app talking to an auth API – they might use mTLS with device certificates for zero trust. Testing that requires knowing the cert setup. Many frameworks (like FAPI in banking) actually mandate mTLS for client auth. So in cross frameworks (like financial API standards), this is required. OWASP ASVS is basically aligning with those high-security scenarios.",
        "sources": [],
        "caveats": [
          "Verifying mTLS in a black box test can be difficult. Usually, lack of evidence doesn’t prove it’s not there because the client certificate could be configured server-side. If the test is white-box or if you have a way to query, do so. Otherwise, you might list it as “could not verify, ensure that mTLS is in place per requirement”.",
          "If the app uses an identity provider like Google or Facebook, mTLS is not applicable because you can’t enforce mTLS with a public IdP. ASVS’s requirement mostly applies to custom or enterprise setups where you control both sides.",
          "Mutual TLS adds complexity (cert management). Some orgs skip it and use other means (like signed JWTs with shared trust). ASVS specifically says mTLS though. If they use an alternate strong mechanism (like private network + API keys), you might mention it but formally mTLS is what's asked. Perhaps note if there’s a compensating control (e.g., VPN between app and CSP) but ideally mTLS."
        ]
      },
      {
        "title": "Verify replay resistence through OTP devices/authentcators or lookup code",
        "asvs": "2.2.6",
        "simple_desc": "Any one-time passwords (OTPs) or verification codes used for authentication must be replay-resistant. This means once an OTP or code is used, it cannot be used again (and ideally expires quickly). Also, authenticator devices (like TOTP apps or hardware tokens) inherently provide replay resistance by generating new codes each time. Essentially, ensure the system doesn't accept an OTP more than once or outside its valid window, preventing an attacker from capturing and reusing an OTP.",
        "references": "- **NIST SP 800-63B** – Requires OTPs to be implemented as one-time (either single-use or limited time window) and some form of replay detection at verifier:contentReference[oaicite:102]{index=102}:contentReference[oaicite:103]{index=103}.\n- **OWASP ASVS** – V2.2.6: specifically calls for replay resistance (no reuse of OTPs or codes).",
        "how_to_test": {
          "manual": [
            "If the application uses one-time codes (for MFA or password reset or any auth step), perform a test: use a code correctly once, then immediately try using the same code again (maybe by submitting it twice, or intercepting the request and replaying it). It should be rejected the second time.",
            "If codes have a time window (like TOTP valid for 30s), try using it once within the window, then again shortly after. The second attempt should fail if the server marks it as used.",
            "For backup codes (lookup codes that are meant to be used once each), use one and then attempt to reuse it. It should be invalid on second use.",
            "If you have network logs, check that each OTP or code is unique per attempt and that the server doesn’t accept duplicates. If possible, also verify the server side by maybe creating two sessions and see if an OTP sent to one can be replayed in another (shouldn’t work)."
          ],
          "tools": [
            {
              "name": "Burp Repeater",
              "use": "Capture a valid OTP submission (say a 2FA code verification). After a successful verification, resend the same request (with same OTP) in Repeater to see if the server rejects it as 'code already used' or 'invalid'."
            },
            {
              "name": "Automation Script",
              "use": "Write a small script that simulates two submissions in quick succession, or uses the same code on two different sessions. This can test if any race condition allows reuse (rare, but worth a try if the architecture allows)."
            }
          ],
          "combo_packs": [
            "Include in **MFA/OOB Security Pack**: along with checking code expiration (2.7.2, 2.7.3) you check one-time use here. For instance, after doing 2.7.3 (one-time use only), you effectively cover replay resistance. They are very related (avoid reuse=replay resistance)."
          ]
        },
        "grouping": {
          "owasp_wstg": "WSTG does not explicitly mention OTP replay, but this is a logical part of testing 2FA implementations – ensure tokens can’t be reused. Some testers do try replaying the same code to see if any flaw allows it (especially if time-based tokens might be accepted multiple times within the time window if server doesn’t track used ones).",
          "ptes": "When testing MFA, a PTES-aligned tester would consider replay attacks. If they intercept an OTP and try to reuse it, and it works, that’s a bug. So it falls under that creative attack testing.",
          "nist_800_115": "Relates to Attack phase techniques - replay attack is a known category. Here we are ensuring the app is not vulnerable to a replay of OTP.",
          "osstmm": "Would classify acceptance of reused OTP as a protocol weakness – failing to enforce one-time usage is a design flaw. OSSTMM teaches to test for such flaws (though not OTP-specific, general replay).",
          "issaf": "ISSAF likely would have a test step for replay attacks in authentication. This is a specific instance; they'd encourage checking that one-time tokens are truly one-time."
        },
        "applicability": "This applies wherever one-time or single-use codes are used in authentication. If the app doesn’t use OTPs at all, then N/A. If it does (for MFA, resets, etc.), must test. Also consider push notifications that require user approve/deny – those aren’t 'codes' but the idea is similar (can't replay an approval if it’s tied to one event). Usually fine.",
        "cross_method_example": "We often test 2FA implementations by trying things like old code replay. For instance, say an app sends a 6-digit code via SMS for login. A pentester will try: use it once to login, then immediately try the same code in a new session – expecting failure. If it somehow worked, that’s a serious vulnerability (someone could sniff an SMS and reuse it multiple times). Standards: ASVS demands one-time use. NIST requires verifiers to invalidate OTP after use or on next code generation. Many real-world breaches involve stealing an SMS code and using it, but reuse isn't usually an issue – it's using it concurrently. Nonetheless, some poorly coded systems might not invalidate server-side if, say, the user enters wrong info first then correct OTP – maybe OTP stays valid? We look for that. It's a pretty universal security concept: one-time means one-time. So all frameworks would agree if an OTP can be reused, it's a flaw.",
        "sources": [
          "https://owasp.org/www-project-application-security-verification-standard/"
        ],
        "caveats": [
          "If your testing environment doesn’t easily allow multiple simultaneous attempts, just do sequential as described. A tiny window exists where maybe if two attempts hit at the same time, both succeed (race condition). This is hard to test manually, but if the risk is critical, you might script it or just mention the ideal (server should have atomic invalidation).",
          "Be mindful that some TOTPs (like Google Auth codes) might be accepted twice if time is on the boundary (like code for previous interval still accepted a few seconds after new one starts). That’s normal tolerance, not a replay vulnerability. We’re concerned with using the exact same code more than once within its valid period or after usage.",
          "When testing backup codes (often given in a set), remember each is one-time. If you test one, you lose it. Keep track and perhaps generate a new set after testing if needed."
        ]
      },
      {
        "title": "Verify intent to authenticate through entry of an OTP token or button press on FIDO hardware",
        "asvs": "2.2.7",
        "simple_desc": "The multi-factor authentication process should ensure user presence/intent. That means the user has to do something interactive (enter a one-time code, or physically touch their security key) to confirm they are authenticating. This prevents scenarios where an authentication could be completed without the user’s active participation (for example, malware triggering a biometric or a stored token without user knowing). Essentially, the second factor must involve user action.",
        "references": "- **NIST SP 800-63B** – Mentions user SHOULD demonstrate intent to authenticate (like requiring a user gesture on security device):contentReference[oaicite:104]{index=104}.\n- **FIDO U2F/UAF specs** – Require a user gesture (touching the key) to prevent silent authentication by malware. ASVS aligns with that philosophy.",
        "how_to_test": {
          "manual": [
            "If using TOTP or SMS codes, the requirement is inherently met because the user must retrieve and enter the code (that shows intent). So just note that if OTP is used, user input of it demonstrates intent.",
            "If using hardware tokens (YubiKey, etc.), test that you actually have to tap the YubiKey or biometric for it to send the credential. Many keys will not send the response unless touched. Ensure that's the case (most FIDO2 keys enforce it by design).",
            "If there's any authentication flow that might automatically proceed (for instance, some password managers can autofill OTPs, or some device stored credentials might auto-send), consider whether user presence is checked. Typically, hardware tokens always require press, so it's fine.",
            "For less common scenarios: e.g., a faceID or fingerprint on phone used as second factor – that inherently requires user biometric (intent). So check that these flows indeed prompt the user for biometric or aren’t happening silently."
          ],
          "tools": [
            {
              "name": "Observation",
              "use": "This is largely a conceptual test. You observe the flows. For FIDO keys, just confirm that you had to touch it (some keys have LED that blinks waiting for touch)."
            },
            {
              "name": "Documentation",
              "use": "Ensure that the configured method is one that includes user presence. If an OTP is delivered and auto-submitted by app without user input, that could violate intent (rare). Possibly check if the app auto-reads SMS OTP on mobile and submits – the user still requested login though. It's borderline but likely fine because user did initiate login."
            }
          ],
          "combo_packs": [
            "Part of **MFA Behavior Pack**: test 2.2.6 (replay) and 2.2.7 (intent) together. E.g., observe that the user had to enter a code or tap something – that covers intent, and earlier you checked that code can't be reused."
          ]
        },
        "grouping": {
          "owasp_wstg": "Not explicitly in WSTG, but it's consistent with good MFA testing. WSTG might mention checking that U2F devices require a press (which is standard). They don't have a direct test for 'user intent', it's assumed in design.",
          "ptes": "PTES wouldn't highlight this on its own. It's more a security property of MFA. Only if an MFA implementation lacked user presence (which would be weird) would a tester notice and report it.",
          "nist_800_115": "No, but mapping to 800-63: at AAL2, at least one factor should demonstrate intent per NIST (like a key touch). It's a design requirement rather than test beyond verifying it functions as intended.",
          "osstmm": "Not addressed specifically. MFA details are beyond OSSTMM’s general scope, but requiring user interaction is simply part of making MFA effective.",
          "issaf": "ISSAF likely doesn't go this deep into MFA internals. It's an ASVS specificity influenced by modern FIDO standards."
        },
        "applicability": "Applies if multi-factor authentication is used and one of the factors could be automated. E.g., for FIDO keys or OTP, ensure there's a user step. If the app only has password (single factor), this is sort of moot (N/A, because there's no second factor). So applicable only when a second factor exists or is expected.",
        "cross_method_example": "This is particularly relevant for FIDO U2F keys: a vulnerability found in early days was theoretical – if malware could trigger an unattended USB token that auto-responds without user touch, that’s bad. The whole design of those keys includes user presence to mitigate that. ASVS just ensures the application doesn’t implement some odd method that bypasses user intent. In real testing, one might say: Yes, the application uses standard TOTP for 2FA, which inherently requires user to input a code, satisfying the requirement of intent. We mark it as compliant. If the application had some custom thing like pushing a second factor silently to a device, we’d investigate. But usually, it's straightforward. So cross frameworks: FIDO Alliance specs ensure this at tech level; ASVS ensures you considered it. Others seldom call it out, they assume proper MFA usage. But calling it explicitly is good as a check item.",
        "sources": [
          "https://pages.nist.gov/800-63-3/sp800-63b.html"
        ],
        "caveats": [
          "Not all second factors are equal – push notifications for example: some just show a notice on your phone and you hit 'Approve'. That's intent (the user clicked). If an app ever just sends a push and auto-approves without user click, that’s a serious flaw. Unlikely, but if you see any auto-approval, definitely report that.",
          "If no MFA in place, mark N/A but maybe recommend MFA introduction. However, this item specifically is about intent for existing MFA or one-time flows, so it’s not about requiring MFA existence (that was 2.2.4).",
          "Sometimes testers ask: does entering password count as demonstrating intent? Well yes, but here we mean the additional step. So don't confuse primary auth with second factor intent. Focus on the second factor usage."
        ]
      }
    ]
  },
  {
    "category": "File Handling (v12)",
    "category_prefill": {
      "asvs": "ASVS v4.0.3 — V12 File and Resources Verification Requirements (V12.1 through V12.6) covering upload, execution, storage, download, SSRF controls.",
      "references": "- OWASP ASVS: Chapter 12 covers secure file upload/download handling:contentReference[oaicite:105]{index=105}.\n- OWASP File Upload Security Cheat Sheet: best practices on file validation, storage:contentReference[oaicite:106]{index=106}:contentReference[oaicite:107]{index=107}.\n- OWASP WSTG: Tests for path traversal and file inclusion (WSTG-ATHZ-01):contentReference[oaicite:108]{index=108}:contentReference[oaicite:109]{index=109}, file upload tests (WSTG-BUSLOGIC-4.10.8/9):contentReference[oaicite:110]{index=110}.\n- NIST 800-53: SA-17 (malicious code checking) and SI-10 (input validation) align with scanning files and constraining uploads.",
      "simple_desc": "This category ensures the application safely handles user-supplied files and file operations. It addresses restricting file upload sizes and types, validating and sanitizing file names/paths, preventing code execution from uploads, scanning for malware, storing files securely (outside web root, with safe permissions), controlling file download types, and preventing server-side request forgery via file handlers. The goal is to prevent file-based attacks like DoS via huge files or zip bombs, arbitrary file reads/writes (path traversal), remote file inclusion, and code execution from malicious files.",
      "how_to_test": "For file upload functionality: attempt to upload files exceeding expected size limits and observe if the app enforces limits (should reject overly large files):contentReference[oaicite:111]{index=111}. Try a known zip bomb (carefully):contentReference[oaicite:112]{index=112} or a large compressed file to see if extraction is mitigated. Upload various file types: allowed types (images, etc.) vs disallowed (executable scripts, etc.) to ensure proper type validation:contentReference[oaicite:113]{index=113}:contentReference[oaicite:114]{index=114}. Test for path traversal in file names by including \"../\" in filenames on upload or parameters that reference files:contentReference[oaicite:115]{index=115} – ensure the application doesn’t save files in unintended locations or allow access to sensitive server files. If the app serves uploaded files, attempt to access them and see if any dangerous types (like .html with script) are executed – a proper implementation will serve them with safe Content-Type or sanitize them. Check if antivirus scanning is in place: e.g., upload the EICAR test string; it should be blocked or removed if scanning works. For file downloads, see if the server restricts access to certain file types or directories (no listing of .bak or config files) and that untrusted file content (like user-uploaded HTML) isn’t served inline to cause XSS. Attempt SSRF via any file fetch functionality: if the app lets you supply a URL to fetch a file (or does server-side PDF generation with external images), try pointing it to internal IPs or external server you control to verify it doesn’t fetch unintended resources. Use combo tests: for instance, upload a file with `<script>` and then access it to check for stored XSS (should be served as a download, not rendered). Combine a path traversal test with an upload to see if you can retrieve a file from the server by tricking file handler. Many of these tests can be grouped to run in one go using a tool like Burp with Intruder or a Fuzzing tool (ffuf, etc.) for directories:contentReference[oaicite:116]{index=116}. Also consider using ready-made rules/templates like **nuclei** for common file handling issues (there are templates for LFI, open file repositories, etc.). Always monitor app responses and server behavior (error messages, etc.) for clues during these tests.",
      "grouping": "owasp_wstg: File issues are tested in sections like Input Validation (LFI/RFI):contentReference[oaicite:117]{index=117} and Business Logic (file upload situations):contentReference[oaicite:118]{index=118}; ptes: Covered during Vulnerability Analysis – testers enumerate file entry points (uploads, downloads) and attempt abuses (traversal, LFI, RFI, file injection); nist_800_115: Discovery phase to find file inputs, Attack phase to try exploiting file handling (attempt uploading malicious files, path traversal, etc.) – part of web app penetration; osstmm: Falls under Data Security (ensuring files from users are properly controlled) and Attack Surface reduction (no unintended file execution or exposure); issaf: In the Web Testing module, there are steps for \"File Upload flaws\", \"Directory Traversal\", and \"Injection via files\" that align with these checks.",
      "applicability": "Relevant for any application that accepts file uploads or serves files dynamically. If the app does not handle user files at all (no uploads, no user file downloads), many of these can be N/A. For instance, SSRF via file upload is N/A if there’s no upload or file-fetch functionality. Mark checks N/A if you have confirmed the functionality is not present (e.g., no upload feature means upload-size, type, scanning checks N/A). When in doubt (if feature might exist hidden, like an admin upload), discuss with developers or attempt to find any file endpoints.",
      "cross_method": "In OWASP Testing Guide and many real pentests, file handling vulnerabilities are a major area – e.g., unchecked file uploads can lead to web shells (Critical). Other frameworks: **OWASP Top 10** includes \"A5: Security Misconfiguration\" which covers leaving files accessible or executing on server, and \"A1: Broken Access Control\" for path traversal. **PTES** would classify a found file inclusion or upload RCE as a critical finding under exploitation. **NIST 800-53** controls require scanning uploads and validating input – mapping to our tests for AV scanning and type whitelisting. A cross-methodology example: If an app has an image upload, OWASP WSTG says try uploading a script with a .jpg extension (test content vs extension handling). OSSTMM would call that a \"Poison File\" test. They all drive at the same point: ensure user files can’t compromise the host or other users. So our tests mirror those from multiple angles.",
      "sources": ":contentReference[oaicite:119]{index=119}:contentReference[oaicite:120]{index=120}:contentReference[oaicite:121]{index=121}",
      "caveats": "File testing can be destructive or resource-intensive – get permission before attempting something like a zip bomb or very large file upload, as it can degrade server performance. Use smaller test files first to gauge behavior. Clean up uploaded test files if possible (so you don’t consume space or leave dangerous payloads around). When testing file inclusion/path traversal, be careful not to retrieve sensitive data beyond scope (if scope says no touching certain files, don’t exploit an LFI to read them without clearance). Also note that some anti-virus or WAF protections might react (triggering alerts) when you upload EICAR or other probes – coordinate with security monitoring teams so they know it’s testing. Finally, if you encounter errors or stack traces during file testing, treat that as an additional finding (information leakage or poor error handling), though that’s tangential to this category."
    },
    "items": [
      {
        "title": "File upload sizes are not restricted",
        "asvs": "12.1.1",
        "simple_desc": "There should be a limit on file upload size to prevent denial of service (filling up storage or overwhelming processing). If file upload functionality doesn’t enforce a reasonable max file size, an attacker could upload an extremely large file to consume space or memory.",
        "references": "- **OWASP Cheat Sheet** – “Set a file size limit” for uploads:contentReference[oaicite:122]{index=122}:contentReference[oaicite:123]{index=123}.\n- **OWASP WSTG** – Testing for DoS via large payloads (implied under input handling; zip bombs etc.):contentReference[oaicite:124]{index=124}.",
        "how_to_test": {
          "manual": [
            "If the application documentation/UI mentions a max file size (e.g., 'Max 5MB'), attempt to upload a file larger than that (say 2x the limit). Observe if the app rejects it (with an error) or if it tries to accept it (progress bar, then maybe fails later or succeeds). If it accepts way beyond advertised limit, that’s an issue.",
            "If no stated limit, try progressively larger files: e.g., 1MB, 10MB, 50MB, etc., to find if a limit exists. Keep an eye on upload time and server response. If a 100MB file uploads and you get a success, likely no proper limit.",
            "Monitor application response: ideally, the server should respond with HTTP 413 (Entity Too Large) or a friendly error if limit exceeded. If it doesn’t, maybe the limit is only client-side (like HTML file input 'max' attribute) – try bypassing by using an intercepting proxy to send a larger payload.",
            "Be cautious not to crash the server – escalate sizes slowly. Also test compressed file if the server might decompress (for zip bomb potential)."
          ],
          "tools": [
            {
              "name": "Curl",
              "use": "Use curl for quick upload tests of large files (if endpoint can be hit directly). E.g., `curl -F 'file=@largefile.dat' https://site/upload` and see response/time."
            },
            {
              "name": "Burp Suite (Intruder/Payload)",
              "use": "Not typical for large binary, but you could use Burp Repeater to send a known large file by pasting raw data or using the upload function in Burp. Burp will show if server cuts connection or not."
            }
          ],
          "combo_packs": [
            "This ties with **DoS & Abuse Pack**: test not only file size but also multiple files (if allowed concurrently) to see if combined volume is limited. And with zip bomb test in next item to gauge protections."
          ]
        },
        "grouping": {
          "owasp_wstg": "Part of testing upload functionality for abuse – WSTG doesn’t have a direct case, but falls under resilience testing (maybe in Denial of Service or input validation).",
          "ptes": "In Vulnerability Analysis, tester might attempt resource exhaustion attacks. Large file upload is one such method. They’d note absence of file size checks as a finding (DoS risk).",
          "nist_800_115": "Part of robustness testing – e.g., Section on DoS testing might include sending overly large inputs to see system behavior.",
          "osstmm": "OSSTMM would consider failure to restrict size as lack of 'limitation control' on an input channel. It invites an availability weakness.",
          "issaf": "ISSAF likely mentions testing application’s ability to handle large inputs (could be memory exhaustion, etc.). They would treat unlimited upload as a test scenario for availability."
        },
        "applicability": "Applies if the application has file upload capability. If no upload feature, mark N/A. If only very small files are logically allowed (like text snippets), still check if the app enforces it. Otherwise, any upload endpoint should have limits.",
        "cross_method_example": "We often see issues where an app says 5MB max but doesn't actually enforce server-side. I recall a pentest where we uploaded a 100MB file to a profile picture upload (expected to be small images). It succeeded and caused server processing lag. That was reported as a potential DoS flaw. Many frameworks (like OWASP Secure Coding guidelines) say always enforce maximum size. The ASVS requirement is basically ensuring that. If this were missed, an attacker could script repeated large uploads to fill storage. So it’s a low-hanging fruit test. Other standards like CWE have an entry (CWE-400 Uncontrolled Resource Consumption) which covers this scenario. So across methodologies: yes, it's recognized as a problem if not addressed.",
        "sources": [
          "https://cheatsheetseries.owasp.org/cheatsheets/File_Upload_Cheat_Sheet.html"
        ],
        "caveats": [
          "Be mindful of what “restricted” means: sometimes there’s a limit in web server config (like max post size). If you try to exceed and the server drops connection, it might be enforced at infra level. If you see that behavior, the control exists but maybe not app-layer. Document it anyway ('server refused after ~X MB').",
          "If the app uses chunked uploads or an async method, detecting limit might be trickier. For chunked, it might accept chunks and only complain at end or not at all. In those cases, try to deduce by total size that got through.",
          "Also consider number of files if multiple uploads are allowed. A single file might have a limit, but you could upload 100 files of near-limit size. That might also exhaust space. So note if there's any aggregate or per-user limit (often not). That might be beyond ASVS explicit scope but relevant in analysis."
        ]
      },
      {
        "title": "[CAREFUL RISK OF DOS] Zip bomb prevention",
        "asvs": "12.1.2",
        "simple_desc": "The application should protect against zip bombs or similar compressed bombs. A zip bomb is a small compressed file that expands massively, potentially filling up disk or memory if naively decompressed. The app should detect and safely handle compressed files to avoid DoS – for example by limiting compression ratios or using safe decompression libraries, or rejecting overly nested/compressed archives.",
        "references": "- **OWASP Cheat Sheet** – Mentions using secure methods to calculate zip size before extraction:contentReference[oaicite:125]{index=125}:contentReference[oaicite:126]{index=126}.\n- **Known Attacks** – The classic 42.zip bomb example (tiny file expanding to gigabytes) is often cited:contentReference[oaicite:127]{index=127} – app should mitigate.",
        "how_to_test": {
          "manual": [
            "Identify if the app processes uploaded zip files (e.g., unzips them server-side for scanning or to extract content). If not (just stores them), risk is lower, might mark N/A. If yes or unknown, test carefully.",
            "Obtain a known zip bomb (like 42.zip which expands to huge size). Ensure you have permission as this can crash the server. Upload it through the interface.",
            "Observe the outcome: ideally, the server should detect and reject it (maybe “Archive too large” or similar). If the server tries to extract and then slows/crashes, that indicates no protection.",
            "A safer alternative: create a zip with a high compression ratio yourself (like a text file of repeating character 100MB compresses to a few KB). Upload that and see if the app handles it quickly or struggles. Monitor response time.",
            "Check logs or error messages if available for signs of 'Decompression failure' or such. If the app is not transparently extracting, it might still scan internally (some virus scanners auto-extract, etc.)."
          ],
          "tools": [
            {
              "name": "Custom zip bomb",
              "use": "Use a tool to create a compressed bomb: e.g., create a file with million repetitive bytes and zip it at max compression to get a huge expansion ratio. There are scripts online to generate gz or zip bombs of chosen sizes."
            },
            {
              "name": "System monitoring",
              "use": "If you have access to monitor server resources (maybe not in black-box), watch memory and CPU during your test. A spike to 100% CPU or out-of-memory when uploading the zip indicates an issue."
            }
          ],
          "combo_packs": [
            "Combine with **Resource Exhaustion Pack**: test large file (previous item) and zip bomb sequentially. They both target DoS via files. The difference is zip bomb is smaller going in, bigger impact on processing."
          ]
        },
        "grouping": {
          "owasp_wstg": "WSTG might not explicitly say 'zip bomb', but it falls under DoS and file upload testing. It’s an attack variant testers should consider.",
          "ptes": "PTES in a DoS context or stress testing context might include this. Typically, not every pentest tries a zip bomb due to risk, but it's known.",
          "nist_800_115": "As part of robustness testing, an advanced tester could include malicious payloads like zip bombs to see if controls exist.",
          "osstmm": "Would view failing this as lack of input sanitization and control – it’s a form of malicious input. OSSTMM focuses on all input should be controlled to avoid unexpected resource usage.",
          "issaf": "If ISSAF covers file upload attacks, it might mention compressed file exploits. It’s not mainstream in older docs, but it’s in modern awareness."
        },
        "applicability": "Only relevant if the application or associated scanner extracts or processes archives. If users cannot upload archives or if the system never inspects inside them, the risk of zip bomb triggering is low (though storage could still blow up if someone uploads a huge compressed file and someone later decompresses offline, but that’s not on the server). So if no archive handling, mark N/A. If uncertain, test anyway carefully or discuss with devs.",
        "cross_method_example": "There was a case where a virus scanning service accepted zips and tried to scan inside them – attackers sent zip bombs to tie up the scanner. So ASVS wants to ensure mitigation. Other frameworks (like OWASP Top 10's availability concerns) indirectly cover this as a DoS vector. In a coordinated pentest, testers might avoid actual zip bombs to not crash production, but in a staging environment they would try. Many modern file libraries have safeguards (max recursion depth, max expanded size). Testing verifies those. It's a rather specific test but important for robust file handling. Many CTFs or bug bounties have had zip bomb challenges – it's known enough to be in OWASP’s list now.",
        "sources": [
          "https://cheatsheetseries.owasp.org/cheatsheets/File_Upload_Cheat_Sheet.html"
        ],
        "caveats": [
          "Absolutely confirm you have a safe environment for this test. Zip bombs can be disruptive. If not sure, you can simulate by asking 'what happens if someone uploads a 1GB zip?' as a question rather than doing it.",
          "There are smaller 'logic bombs' like an XML bomb (billion laughs) – this item specifically calls zip, but similar concept for XML or other compressed formats. If the app parses XML from uploads, consider that too.",
          "If your upload interface automatically rejects .zip or large files, a zip bomb might not even get processed – still, note whether that is due to type checking or something else."
        ]
      },
      {
        "title": "Limits on how many files a user can upload (e.g. rate limit)",
        "asvs": "12.1.3",
        "simple_desc": "There should be controls on the quantity and frequency of file uploads per user (or per session) to prevent abuse (like a user flooding the server with thousands of files). If unlimited, an attacker could try to consume storage or overwhelm the system by rapid-fire uploads. Rate limiting or quantity quotas mitigate this.",
        "references": "- **OWASP Cheat Sheet** – Suggests considering user quotas and authorization level checks on file uploads:contentReference[oaicite:128]{index=128}:contentReference[oaicite:129]{index=129}.\n- **General Best Practice** – Many platforms cap number of uploads per user or implement captchas after X uploads.",
        "how_to_test": {
          "manual": [
            "If the app UI restricts number of files (e.g., 'you can upload up to 5 files'), try to bypass if possible or confirm it’s enforced server-side by attempting a 6th upload. See if server rejects it or if it goes through (if it goes through, server isn't enforcing).",
            "If no explicit UI limit, attempt to upload files in quick succession. For example, upload 10 or 20 small files rapidly (this can be manual rapid clicks or automated). Observe if at some point you're blocked or if performance degrades. Also look for any error like 'too many uploads' or a temporary ban.",
            "If the app has an account storage quota (like max X MB or Y files per user), try to exceed it. E.g., if each user has 100MB limit, upload 101MB total across multiple files and see if the app complains or stops further uploads.",
            "Additionally, see if uploading is tied to user authentication such that an attacker can’t easily script anonymous uploads. If it’s open, then rate limiting should be at IP level as well.",
            "This can be time-consuming to test thoroughly, so prioritize checking for any evidence of controls (like messaging or config). If none, at least note that you were able to upload a large number without restriction."
          ],
          "tools": [
            {
              "name": "Burp Intruder or Turbo Intruder",
              "use": "Automate multiple upload requests. For instance, capture a file upload request and then use Intruder to repeat it, changing the file name each time (or the file content). Monitor responses – if after a certain number you get different status (like 429 Too Many Requests or some block), that indicates rate limiting in effect. If all succeed, likely no limit."
            },
            {
              "name": "Script (Python requests)",
              "use": "Write a loop to upload, say, 50 files sequentially via the API or form, and record results (HTTP codes, time). If the 51st fails or slows drastically, that’s a sign. If all 50 succeed quickly, likely no enforcement."
            }
          ],
          "combo_packs": [
            "Combine with **Abuse & Spam Pack**: test uploading spam. E.g., a malicious user might try to upload 1000 tiny files. See if the app has any anti-spam measures. This is akin to checking messaging/API rate limits but for file endpoints."
          ]
        },
        "grouping": {
          "owasp_wstg": "Falls under rate limiting and abuse prevention – WSTG has sections on rate limiting (usually for functional endpoints). This is just applied to file upload endpoints. Not a distinct WSTG test, but a general principle extended to files.",
          "ptes": "In threat modeling, one would ask 'can a user overload storage?'. If yes, that’s a threat to address. A PTES pentester might try to exploit that by automation. So it’s within scope of vulnerability identification (resource exhaustion).",
          "nist_800_115": "This aligns with trying to cause conditions for DoS as part of testing. A NIST-based approach might not list 'upload flood' specifically, but it’s a variant of flooding an interface with requests.",
          "osstmm": "Under process/volume controls, OSSTMM would expect the application to have limits on repetitive actions. If unlimited, it’s a weakness in operational controls.",
          "issaf": "ISSAF likely would mention checking for misuse of functionality – uploading repeatedly is one such misuse if not controlled."
        },
        "applicability": "If file uploads exist, this applies. If no upload function, N/A. If uploads are very limited by design (like one profile picture per user), that inherently restricts number, but test if someone could find another way (like repeatedly updating it may not store multiple but could still cause churn). Usually, focus where multiple uploads are allowed (e.g., attachments in a ticket system).",
        "cross_method_example": "Think of cloud storage or a forum where users can upload files. Without limits, an attacker script could upload millions of small files, potentially filling up storage or at least making a mess. ASVS highlights having some limit. Implementation might be: 'no more than 10 files per minute' or 'each user max 500 files total'. We test to see if those exist. Another viewpoint: We find no evidence of limits; we mention that as a risk. Many bug bounty reports exist where a user could create unlimited resources and cause backend issues – often labeled as potential DoS. Cross frameworks: OWASP Top 10 A6 (Denial of Service) could cover this scenario. It's more about robust design than a vulnerability exploit, but still important. So all methodologies would agree that absence of any throttling is a weakness to note.",
        "sources": [
          "https://cheatsheetseries.owasp.org/cheatsheets/File_Upload_Cheat_Sheet.html"
        ],
        "caveats": [
          "Distinguish between application-level restrictions and infrastructure ones. Maybe the app itself doesn’t block, but the server may have a web application firewall or upload bandwidth throttle. If you hit an unexpected cap (like after 100 requests, responses slow), it could be external rate limiting. You might not always tell from black-box, but mention if you suspect it.",
          "Be careful if testing in a shared environment – flooding with files could annoy admins or trip alarms. Coordinate if you plan to do high-volume testing. Often, stating the lack of explicit limits might be enough if you’ve done moderate testing to confirm it's likely unlimited.",
          "Also consider multi-user scenario: one user might be limited, but an attacker could create many accounts to bypass per-user limits (if any). That goes beyond just this requirement, but for completeness you can mention that per-user quotas help but distributed attack is still a consideration (which might need overall system capacity planning, not in ASVS scope)."
        ]
      },
      {
        "title": "Lack of validation on file types",
        "asvs": "12.2.1",
        "simple_desc": "The application should validate the type of files uploaded (by extension, MIME type, and ideally content) to ensure only expected/allowed file formats are accepted. If there’s no validation, attackers could upload disallowed or dangerous file types (like scripts, executables) possibly leading to security issues (like stored XSS if an HTML is served, or malware on download). So lack of type validation is a vulnerability.",
        "references": "- **OWASP Cheat Sheet** – Emphasizes whitelisting allowed file extensions/MIME and validating file signatures:contentReference[oaicite:130]{index=130}:contentReference[oaicite:131]{index=131}.\n- **OWASP ASVS** – V12.2.1 requires ensuring only permitted file types get through.",
        "how_to_test": {
          "manual": [
            "Review what file types are expected. If it’s an image upload feature, likely only images should be allowed. Try uploading a file with a different extension (e.g., a `.jsp` or `.php` disguised maybe by changing extension to `.jpg`). Does the app block it or accept it?",
            "Test the MIME type: some apps validate by extension only. Use a tool or proxy to change the Content-Type header of the upload to something incorrect or to none and see if it matters. Also, try renaming an executable file to have an allowed extension and see if the content sniffs through. E.g., rename a `.exe` to `.png` and attempt upload – does the app detect by scanning content or just trust the extension?",
            "If possible, after upload, see how the app treats the file: if you managed to upload an HTML or JS file, can you retrieve it via the app? If yes and it’s served as HTML, that’s a problem (XSS or defacement potential). A well-configured server might force download or sanitize these, but the fact it got stored is already not ideal.",
            "Also test double extensions (e.g., `shell.php.jpg`). Some naive filters only check last extension. The server might save it as such; if on a Windows/IIS it might execute `.php.jpg` as PHP due to configuration – rare but known. See if such sneaky names are allowed or not.",
            "Record which files get accepted vs rejected. If literally everything you try is accepted (including clearly off-limits types), then there’s no validation. If some things are blocked, examine the pattern (e.g., blocks `.exe` but not `.phps` or unknown extension)."
          ],
          "tools": [
            {
              "name": "Burp (manual request edit)",
              "use": "Intercept the upload request. Change file name and Content-Type to something inconsistent or malicious (like name=`file.jsp` with Content-Type `image/jpeg`). Forward and see response. Also remove Content-Type to see if server infers type purely by name."
            },
            {
              "name": "OWASP ZAP (Active scan with file upload addon)",
              "use": "If available, some scanning plugins attempt to upload a benign script to test filtering. Alternatively, a Fuzzer in ZAP could try uploading various extension files from a wordlist to quickly map allowed vs blocked types."
            }
          ],
          "combo_packs": [
            "Combine with **Malicious File Pack**: Alongside type testing, you likely also test content (like embedding scripts) and path traversal. For example, an HTML file upload (if allowed) can lead to XSS when accessed – which intersects V12.5.2 (XSS via files). So these tests feed into multiple findings."
          ]
        },
        "grouping": {
          "owasp_wstg": "WSTG has 'Test File Extensions Handling':contentReference[oaicite:132]{index=132} which is exactly this: ensure disallowed extensions aren’t accessible. Also 'Test Upload of Unexpected File Types':contentReference[oaicite:133]{index=133} in Business Logic testing.",
          "ptes": "During vulnerability analysis, testers will definitely check if they can upload web shells or unexpected files. If type validation is lacking, that’s a huge hole typically exploited for RCE or stored XSS.",
          "nist_800_115": "In practice, yes, testers guided by 800-115 would do input validation checks on file inputs which includes type checking.",
          "osstmm": "Under 'Input Validation' part of OSSTMM, file type is another input. Not explicitly spelled out, but fits general principle of whitelisting and not trusting user input.",
          "issaf": "ISSAF would instruct to try uploading various file formats to see if anything dangerous can be uploaded. It’s a common test."
        },
        "applicability": "Applies if file uploads exist. If no uploads, N/A. Also, some apps deliberately allow arbitrary file types (like a cloud storage service). In those cases, this might seem N/A, but even then they often have a list of disallowed types or scan them. But if truly anything is allowed by design, then highlight that risk (some might mark N/A but better to recommend scanning at least). For a typical app where only certain formats are needed, it definitely should enforce it.",
        "cross_method_example": "This is classic – many breaches started with an upload that wasn’t properly filtered (leading to web shells). ASVS sees 'lack of type validation' as a big no-no. For instance, an attacker finds they can upload `.aspx` file to a site because the site only checked that filename contains '.asp' not '.aspx' – then they execute it. Other frameworks: CWE-434 (Unrestricted File Upload) addresses this broadly. OWASP Top 10 (Injection or broken access control) could encompass it when execution is achieved. So virtually all methodologies require content validation. On a pen test, if I can upload a `.html` or `.php` where only images expected, I will escalate that to see if I can execute or deliver XSS. The presence of that condition itself is a finding to report.",
        "sources": [
          "https://cheatsheetseries.owasp.org/cheatsheets/File_Upload_Cheat_Sheet.html",
          "https://owasp.org/www-community/vulnerabilities/Unrestricted_File_Upload"
        ],
        "caveats": [
          "File type validation can be bypassed if only done client-side or by extension. We test server-side. Also, just because an app says 'file uploaded' doesn’t mean it’s dangerous unless you can do something with it (like retrieve it or execute it). But from a security stance, storing unexpected files is still a risk (malware repository etc). So note it even if immediate exploit isn't obvious.",
          "Some frameworks handle a lot for you – e.g., SharePoint renames .aspx to .aspx.txt on upload. If you see behavior like renaming or altering content, that’s a defense (content neutralization). Recognize if such things happen.",
          "Be thorough: there's file content vs extension mismatches. Some apps check magic bytes of file. If they do, good. If not, you can upload a script renamed as .jpg and maybe later change extension via some trick to execute. Harder, but lack of content check is worth noting. However, ASVS specifically states allowed types, implying extension/MIME filtering at least."
        ]
      },
      {
        "title": "Path traversal through user input (e.g. file names)",
        "asvs": "12.3.1",
        "simple_desc": "The application must prevent directory/path traversal attacks, where an attacker provides file paths with `../` sequences or absolute paths to access files outside intended directories. If user input (like file name or path parameter) is used to read or write files on the server, it should be sanitized to avoid traversals. A lack of protection could allow reading sensitive files or saving uploads in unintended places.",
        "references": "- **OWASP WSTG** – Testing for Directory Traversal/File Include (WSTG-ATHZ-01):contentReference[oaicite:134]{index=134}:contentReference[oaicite:135]{index=135}.\n- **OWASP Cheat Sheet** – Filename Safety: recommends restricting characters like `../` and normalizing paths:contentReference[oaicite:136]{index=136}:contentReference[oaicite:137]{index=137}.",
        "how_to_test": {
          "manual": [
            "Identify any functionality where a filename or path from user is used. Common cases: file download links (like `download.php?file=report.pdf`), image fetching by name, or even hidden fields in upload forms that contain a path.",
            "Attempt to manipulate that parameter with traversal sequences. For example, if `file=report.pdf`, try `file=../config.php` or absolute paths like `file=C:\\windows\\win.ini` (on Windows) or `/etc/passwd` (on *nix). Observe the response: if you get content of those files or any error referencing them, it's vulnerable.",
            "Also try URL encoding the `../` (`%2e%2e%2f`) and other encodings to bypass naive filters, as well as double traversals (`....//` which decodes to `../` in some cases).",
            "If uploading files, see if file names with `../` in them cause files to be stored in parent directories. E.g., name an upload `../evil.php` and check server for where it ended up (hard in black box, but maybe error or directory listing might show if it escaped the upload folder).",
            "Be cautious not to break things by overwriting if the app uses user input in file writes. But usually reading is the bigger risk (LFI – Local File Inclusion). If the app is showing an image by filename, trying traversal might either show some system file content or error out – both informative. Document any successes or error messages (like file path disclosures)."
          ],
          "tools": [
            {
              "name": "OWASP ZAP (Forced Browsing or File Inclusion scanner)",
              "use": "ZAP has some built-in checks for LFI. You can input a URL with a parameter and ZAP active scan will try traversal payloads. Similarly, Burp Scanner does that. These can automate trying multiple patterns and see if known file signatures (like presence of 'root:' from /etc/passwd) appear in responses."
            },
            {
              "name": "PayloadsAllTheThings list",
              "use": "Use a list of common traversal payloads (../, ..\\, .../ etc.) and try them manually or via Intruder. This ensures you test various encoding bypasses."
            }
          ],
          "combo_packs": [
            "Combine with **File Inclusion Pack**: Test not only traversal (which is reading unintended files) but also any possible RFI if parameters accept URLs, and LFI to RCE (like if there's an LFI in a PHP include context, could possibly include a session or log file with PHP code for RCE). That's advanced but part of same family."
          ]
        },
        "grouping": {
          "owasp_wstg": "WSTG-ATHZ-01 covers path traversal thoroughly:contentReference[oaicite:138]{index=138}:contentReference[oaicite:139]{index=139}. It's a core web vul. They instruct to try these payloads.",
          "ptes": "PTES falls under file system testing in Vulnerability Analysis. Testers will systematically attempt to break out of file paths wherever user input is involved in file access.",
          "nist_800_115": "Yes, path traversal is a well-known attack vector – testers guided by common attack patterns will check for it. It's often in any standard test checklist.",
          "osstmm": "Would classify as failure to properly sanitize input affecting file system – definitely a security hole under data validation.",
          "issaf": "ISSAF lists Directory Traversal as a common web app vuln to test for. So yes, it aligns with that."
        },
        "applicability": "Applies whenever user input is used in file path construction. If the application never takes a file name or path from user (like all files are fixed or no file access features), then N/A. But many apps do have some file serving or file param (download, image, etc.), so always check.",
        "cross_method_example": "A famous example: a web app had `page.php?template=about.html`. Attackers tried `template=../../../../../etc/passwd` and it worked, exposing system password file. Or another, a download script didn't sanitize file names, allowing reading config files with `..` sequences. This vulnerability appears in OWASP Top 10 (A5 Broken Access Control often, since it's unauthorized file access). CWE-22 covers it. It's universally recognized. On any pentest, path traversal is one of the first things to try on file parameters. So ASVS including it is basic. We test it the same way across frameworks – try to break out, see if we can access sensitive data or escalate to code execution (if file contains code and is later executed etc.).",
        "sources": [
          "https://owasp.org/www-project-web-security-testing-guide/latest/4-Web_Application_Security_Testing/05-Authorization_Testing/01-Testing_Directory_Traversal_File_Include",
          "https://cheatsheetseries.owasp.org/cheatsheets/File_Upload_Cheat_Sheet.html"
        ],
        "caveats": [
          "Be mindful of file system structure differences: on Windows vs Linux, traversal payload needs slashes accordingly. Some filters block `../` but not `..\\`, etc. Try both if tech stack is unknown. But careful: mixing slashes might not work if code normalizes them.",
          "Sometimes an app might sanitize but poorly. For example, removing `../` but not `....//`. Pay attention to subtle outcomes; if one weird payload returns slower or a different error, that could indicate partial filtering but still vulnerability.",
          "If you suspect traversal but can’t confirm (maybe the app returns generic error for everything missing), you might try to cause a noticeable effect (like triggering a delay by accessing a special device file). That’s advanced and usually not needed – often you'll either get file content or a clear error that shows it tried (like an error saying “file not found /etc/passwd” reveals it attempted to fetch it, meaning traversal succeeded but file wasn’t served)."
        ]
      },
      {
        "title": "Local file inclusion through user input",
        "asvs": "12.3.2",
        "simple_desc": "Ensure that user input cannot be used to cause the server to include or execute local files (like .php, .config, etc.) in ways unintended. Local File Inclusion (LFI) often refers to when an application takes a filename and `include()`s it in code (common in PHP), allowing an attacker to run arbitrary local files as code or expose their content. This is a step further than just reading (traversal) – it’s about executing or inserting the content into output as code.",
        "references": "- **OWASP WSTG** – Same directory traversal tests often cover LFI by trying to include script files or resources:contentReference[oaicite:140]{index=140}:contentReference[oaicite:141]{index=141}.\n- **OWASP Top 10** – LFI is a type of Injection or broken access control depending on context, known high impact vuln if present (e.g., PHP LFI to RCE by injecting logs).",
        "how_to_test": {
          "manual": [
            "LFI typically concerns dynamic file execution. If the app has a parameter like `page=home` that it includes as `home.jsp` or `home.php` on server, try tampering: e.g., `page=../../../../etc/passwd%00` (the %00 null byte trick is for older PHP to terminate string). If code execution context, this might just show file content or error out in an interesting way.",
            "Try to include known application files: for instance, `page=index.php` (the main script) or any file that you know exists on server. See if its content appears (which it shouldn't) or if the app outputs errors with code fragments. That indicates LFI.",
            "If a file you include has some exploitable content (like, including a log file that contains attacker-controlled data can lead to RCE), it's complex but possible. For testing, see if you can get the app to include itself or a config file with credentials (that might print DB passwords).",
            "Similar to traversal, test with various encodings, null byte (%00) for older languages (some languages now ignore null termination, but PHP <5.3 had issues). Also test known LFI patterns like adding a null extension if the app appends an extension automatically (like `page=../../shell%00.png` if it appends `.php`).",
            "Check if any known parameter is directly passed to functions like include/require (if white-box or from error messages). A common sign: error message saying 'failed to include ...' with your input in it."
          ],
          "tools": [
            {
              "name": "Burp/ZAP Intruder (same as traversal)",
              "use": "Use payload list focusing on typical LFI payloads (with null bytes, with common sensitive files etc.). There are lists specifically for LFI fuzzing that you can load. Then analyze responses for clues (especially differences in response length or presence of keywords from files like 'root:' for /etc/passwd)."
            },
            {
              "name": "lfimap or commix specialized tools",
              "use": "There are some specialized scripts (like 'LFISuite') that automate LFI finding and exploitation. They try various tricks to escalate LFI to RCE as well (like injecting into logs). Use with caution, and preferably in a controlled environment, as they might do noisy stuff."
            }
          ],
          "combo_packs": [
            "Pairs with traversal tests. Essentially, if traversal is possible and the file gets executed or included in page output, it's LFI. Combine with attempts to include dynamic files: e.g., if you can upload a file and then use LFI to include it, that can lead to RCE – that's a combined attack chaining upload and LFI."
          ]
        },
        "grouping": {
          "owasp_wstg": "WSTG-ATHZ-01 covers both path traversal and local file include (they often overlap):contentReference[oaicite:142]{index=142}. The difference is subtle (LFI often meant in PHP context including code). WSTG just says check for any file retrieval/inclusion possibility.",
          "ptes": "Yes, as part of injection flaws. LFI can lead to RCE, so definitely sought after in pentests. If an LFI is found, a PTES tester will try to escalate it by either finding a sensitive file or turning it into code execution (for example via poisoning an Apache log with PHP code and including it).",
          "nist_800_115": "Would be seen as part of input validation testing. Not named explicitly but it’s a known attack pattern to test.",
          "osstmm": "Same as traversal, considered a lack of enforcement on input that touches file system. Would be high severity because it can compromise system integrity.",
          "issaf": "ISSAF covers file inclusion vulnerabilities as well; they instruct testers of web apps to try LFI/RFI if applicable. It's common in older PHP apps."
        },
        "applicability": "Relevant if the app is likely to include user-specified files. Mostly in scripting languages like PHP, or maybe server-side template inclusion. If the app is purely static or uses framework that doesn’t allow file name manipulation, it might be N/A. But often, any file download or page parameter suggests a potential. We attempt it broadly if any hint of file paths. If truly not applicable, mark N/A.",
        "cross_method_example": "LFI was behind some big incidents historically (e.g., certain PHP apps where you could include config.php and get DB creds, or include /proc/self/environ and inject code). ASVS wants to ensure no user input goes unchecked into file includes. Other frameworks categorize it under injection or broken access control. A tester will treat LFI as critical if found, because it's often a pivot to deeper compromise. This item is basically 'no LFI vulnerability'. Testing it aligns with general approach to file path manipulations. If code isn't susceptible, our tests yield nothing. If it is, we likely get very obvious results (like sensitive file output).",
        "sources": [
          "https://owasp.org/www-project-web-security-testing-guide/latest/4-Web_Application_Security_Testing/05-Authorization_Testing/01-Testing_Directory_Traversal_File_Include"
        ],
        "caveats": [
          "If you find a file inclusion, consider the scope carefully. Reading certain files might exceed scope (like /etc/shadow). Usually listing /etc/passwd is okay to demonstrate, but be cautious retrieving something with personal data. Stop at proving the vulnerability with a benign file or known public file.",
          "Exploiting LFI to RCE often involves writing to logs or upload combined with LFI. That might be beyond what you need to do in a normal test. Unless specifically allowed for exploitation, you might just report the possibility rather than actually dropping a shell via LFI (which could be disruptive).",
          "Some frameworks like Java rarely have LFI issues because you can't easily include arbitrary files as code. It's more a PHP/Node thing. So consider the tech: if not applicable (like in .NET MVC, views aren't typically file-includable by user), you can mark N/A. But always double-check if there's any custom file serving functionality."
        ]
      },
      {
        "title": "Remote file inclusion through user input",
        "asvs": "12.3.3",
        "simple_desc": "The application must not allow user input to specify a remote file (via URL) to be included or fetched and executed on the server. Remote File Inclusion (RFI) is when an app accepts a URL (like `http://evil.com/shell.php`) and includes it as code or data. This can lead to immediate server compromise (if code is included) or data leakage. Typically an issue in languages like PHP with `allow_url_include` or poorly handled URL fetch functionality.",
        "references": "- **OWASP WSTG** – Also covered in File Include testing (trying to include external URLs):contentReference[oaicite:143]{index=143}.\n- **PHP Security** – RFI was common in older PHP apps; modern configs disable it by default, but custom code might still fetch remote content unsafely.",
        "how_to_test": {
          "manual": [
            "Find any functionality that might accept a URL or domain from user. For example, an app might have a parameter `?feedUrl=http://example.com/feed.xml` to import something, or a debug endpoint where you provide a URL to fetch data.",
            "Try supplying a URL to a server you control (like a request bin or your own web server) to see if the app attempts to fetch it. If it does, that's a SSRF (Server-Side Request Forgery) which is related. But RFI specifically is about including remote content as code – hard to detect externally unless error messages or results show it.",
            "For PHP, test `page=http://yourserver/shell.txt` (with shell.txt containing something benign or distinctive). If the app is vulnerable, it might execute or show the content of that file.",
            "Monitor your server logs to see if the app makes a request out. If it does, at least there's SSRF. If output of that file appears in the application response or triggers behavior, it might be RFI if executed as code (though if it just prints, might be treating it as data fetch).",
            "Look out for error messages like 'failed to open stream: http wrapper...' which indicate it tried to include a URL and failed (common PHP warning if allow_url_include is off but code attempted it). That confirms an attempt and thus vulnerability in code (even if config stopped it)."
          ],
          "tools": [
            {
              "name": "Burp Collaborator",
              "use": "If no your own server, use Collaborator: in any suspect parameter, put a collaborator URL. If the server tries to fetch it, you'll get a ping. That reveals SSRF. Combine that knowledge with context – e.g., if it's a file include, it's RFI attempt."
            },
            {
              "name": "Request Bin / Netcat",
              "use": "Set up a netcat listening on some port and have the app fetch a URL to your machine (requires exposing it) to catch raw request and see what's being asked. Or a simple Python HTTP server to log incoming GET requests."
            }
          ],
          "combo_packs": [
            "This is basically SSRF but targeted at file includes. Can be bundled in **External Interaction Pack**: along with SSRF (12.6.1), test RFI. Actually 12.6.1 SSRF includes remote file fetch, so there's overlap. Test them together – any param that might call out, test internal (SSRF) and external (RFI) addresses."
          ]
        },
        "grouping": {
          "owasp_wstg": "Yes, 'Testing for File Include' includes trying external URLs:contentReference[oaicite:144]{index=144}. It's often tested alongside LFI.",
          "ptes": "Would be identified in vulnerability analysis when testing file handling functions. If found, it's high severity (remote code execution likely).",
          "nist_800_115": "Yes, as part of injection testing (if remote resources can be injected). Also falls under network interaction (since it involves external communication).",
          "osstmm": "OSSTMM would consider that a failure in controlling communication and trust boundaries – the server trusting an external file is a big no.",
          "issaf": "Certainly included in checks for file inclusion vulnerabilities. They likely instruct to test including a remote file if a param is suspect."
        },
        "applicability": "Primarily for applications in languages that can include remote files (PHP with allow_url_include on, maybe some templating engines that fetch remote templates). If the app doesn't have any functionality that accepts URLs from user, N/A. Also, if environment disables remote includes (which modern PHP does by default), the risk is lower, but the code could still attempt it, so still a bug in code if present (just mitigated by config).",
        "cross_method_example": "RFI used to be one of the most critical web flaws, leading to numerous mass exploits in old PHP apps (like the infamous phpBB RFI exploit). ASVS wants to ensure no user input is ever directly fetched as code. In 2025, it's less common due to default settings, but not impossible. The test for it is similar to SSRF: providing a URL and seeing if the server touches it. Frameworks treat it as extremely dangerous (basically code injection + SSRF in one). Many vulnerability scanners have signatures for attempts. If I discovered an RFI, I'd likely prove I can run arbitrary PHP by hosting a simple script and including it – which is game over. That’s exactly how wormable exploits happened historically. So any methodology sees RFI = critical. We test diligently for it if context is there.",
        "sources": [
          "https://owasp.org/www-project-web-security-testing-guide/latest/4-Web_Application_Security_Testing/05-Authorization_Testing/01-Testing_Directory_Traversal_File_Include"
        ],
        "caveats": [
          "Direct RFI is rarer now, because of default settings. More often, you'll encounter SSRF where the app fetches remote data but doesn't execute it. Still, you report that under SSRF. True RFI (executing remote code) you often confirm by an action (like your remote file triggers some effect or content is in output). Confirm carefully to avoid just claiming it without proof.",
          "If testing on production, be VERY careful with RFI attempts – including a remote file that executes can compromise the server. Only test benignly (like including a harmless text or your collaborator which just triggers a DNS lookup). Understand client if allow_url_include is off, code will just error – which you might see in logs but maybe not externally. So sometimes you infer vulnerability by an error message rather than actually executing payload (which is safer).",
          "If you suspect RFI but cannot fully test (due to risk), you might mention it's important to double-check code isn't doing that. Possibly recommend code review or config review to ensure allow_url_include false and no user input passed to file-get operations."
        ]
      },
      {
        "title": "Reflective File Download",
        "asvs": "12.3.4",
        "simple_desc": "The application should not be vulnerable to Reflected File Download (RFD). RFD is a technique where an attacker crafts a link that causes the server to reflect user input in a downloadable file, tricking the user into running malicious content thinking it’s trustworthy. For example, the server might return content with a `Content-Disposition: attachment; filename=\"...\"` header including user-supplied data that can be turned into a .bat file or similar. We need to ensure that downloadable file content or names cannot be manipulated to execute code on the client side.",
        "references": "- **Reflected File Download Whitepaper** – Oren Hafif's research outlines how an attacker can inject payload into a file download:contentReference[oaicite:145]{index=145}:contentReference[oaicite:146]{index=146}.\n- **OWASP Top 10 (A8:2017-Insecure Deserialization)** – Though RFD is more social engineering, it's akin to forcing file execution on client side. Some references categorize it separately.",
        "how_to_test": {
          "manual": [
            "Identify any functionality where the app generates a downloadable file based on user input. Common case: export to CSV/Excel of user data with some parameters (like date range or name included in file). Or perhaps a debug endpoint that dumps data given a parameter.",
            "Test if you can control the file name or file contents via parameters. For file name: see if any parameter is reflected in the `Content-Disposition` header or suggested filename. If yes, try adding an extension like `.html` or `.bat` in that parameter to see if the downloaded file has that extension (e.g., `reportName=evil.bat`). If it does, the user might double-click thinking it's data but it's actually an executable or script.",
            "Test content: if any user input is included inside the file, can you inject a script or command? For example, if the app creates a .csv containing some user-supplied fields, put a payload that could execute in context (like formulas in Excel can execute commands - CSV injection). Or .bat commands if the file could be renamed .bat.",
            "A key is that the link or action comes from a trusted domain, making the user think it's safe. So see how an attacker could craft a URL with malicious parameter values that results in a dangerous file. If parameters are not sanitized, you might inject a classic payload like a Windows command in a .bat or HTML/script if .html.",
            "Observe the downloaded file: does it contain your injection literally? If you manage to get something like `cmd /c calc` or `<script>alert(1)</script>` inside a downloaded file that a user might run or open, it's RFD territory."
          ],
          "tools": [
            {
              "name": "Burp",
              "use": "Intercept the download response. Check headers and content. Modify request parameters to include something like `file=.bat` in filename or a simple script in content and see if reflected. Burp Decoder can help verify content if it's binary or encoded."
            },
            {
              "name": "Twingate blog example code",
              "use": "Not a tool, but referencing known RFD payloads: e.g., Oren’s method often used JSON responses with a callback parameter. If the app has a JSONP endpoint (callback= functionName), that could be exploited to force a .js file download. So look for JSONP. Use known patterns from research to guide what to try where."
            }
          ],
          "combo_packs": [
            "Relates to **Content Injection Pack**: along with testing XSS and content reflections, consider file downloads as an attack surface. E.g., test CSV injection (if generating spreadsheets) which is a subset of RFD concept (not exactly RFD but similar user end exploitation)."
          ]
        },
        "grouping": {
          "owasp_wstg": "Not explicitly in WSTG because RFD was identified later (around 2014). But WSTG sections on testing content reflection and header injection somewhat cover pieces (like checking Content-Disposition handling).",
          "ptes": "Likely not in older PTES docs, but a modern PTES-minded tester would include RFD in their test plan as it’s a known technique, especially if the app has JSONP or file downloads with user input.",
          "nist_800_115": "Would be an advanced finding from testing how the application handles output encoding and file generation. Not spelled out, but falls under injection or output encoding issues.",
          "osstmm": "No direct mention, but OSSTMM’s approach to output handling would flag if user-supplied data is returned in a download without proper sanitization.",
          "issaf": "Not likely explicitly covered given it’s somewhat niche, but a thorough tester following ISSAF would still catch that user input appears in file content or name in a dangerous way."
        },
        "applicability": "Only applicable if the app generates downloadable files using user input. If not, then N/A. It's fairly specific: many apps have some report or export feature though. JSONP endpoints (if any) also should be considered because those can be abused to deliver a 'file'. If none of that, then no worry.",
        "cross_method_example": "Oren Hafif’s original example: by exploiting a common JSONP endpoint, he made the server respond with a file that the browser thought was a .scf (Shell Command File) and the user’s double-click triggered code. It's a clever mix of client and server trust. ASVS included it to ensure awareness. Many devs still haven't heard of RFD, so testers bringing it up is valuable. It’s partially social engineering (convince user to run the file), but the vulnerability is the server enabling it. Other frameworks don't explicitly list RFD; it's relatively new and specific. But it's an amalgam of header injection, content injection, and improper output handling. So it intersects with categories like output encoding (for ensuring no malicious content goes out) and secure headers (forcing safe file types).",
        "sources": [
          "https://www.twingate.com/blog/glossary/reflected%20file%20download"
        ],
        "caveats": [
          "RFD often requires chaining issues and user interaction. When reporting, clarify the scenario: e.g., 'Attacker could send victim a link, victim downloads a file named invoice.bat from your site, thinks it's legit and runs it, executing attacker code.' It's not as straightforward as XSS but still serious especially if your users trust your downloads.",
          "Testing fully might involve creating a POC file that actually does something on open (like a calc pop via .bat or .hta). Only do that in a safe environment (and clearly don’t send such to real users!). Just opening the download to see if it matches your payload is enough to confirm the vuln; you don't have to actually run it.",
          "If the file content is not directly controllable but the name is, an attacker might still trick a user by extension. E.g., file content benign but name `.docx` is actually a `.exe` disguised. Windows might warn if extension doesn't match content type though. Consider how realistic the exploit is: you might note if SmartScreen or other protections would catch it. But since ASVS asks, you report the weakness if present regardless."
        ]
      },
      {
        "title": "OS-level Command Injection Through File Upload (e.g. in file name)",
        "asvs": "12.3.5",
        "simple_desc": "The file upload functionality must not allow OS command injection via file metadata (like file name) or processing. E.g., if the server uses the file name in a system call (like calling ImageMagick convert or a shell script), an attacker could craft a malicious file name such as `image.jpg; rm -rf /`. If not properly handled, the server might execute that trailing command. Essentially, the app should safely handle file names and not pass them unsanitized to OS shell commands.",
        "references": "- **OWASP Cheat Sheet** – 'Filename Safety' and general injection prevention:contentReference[oaicite:147]{index=147}:contentReference[oaicite:148]{index=148} (recommend not using dangerous chars in filenames, etc.).\n- **ImageMagick \"ImageTragick\" CVE-2016-3714** – Example where image file name with special pipe characters led to command execution by convert utility.",
        "how_to_test": {
          "manual": [
            "Try uploading a file with a suspicious name: e.g., `test$(whoami).txt` or `test;uname -a`. Many servers will sanitize or flat-out reject special characters. See how the app handles it – if it renames the file (good) or allows it unchanged (hmm). Then see if any outcome occurs (the attack might not be obvious from outside – no direct feedback, unless you can see files saved).",
            "Monitor application behavior after such upload. If there's any processing (like generating thumbnails), check if any anomalies occur (like error messages or strange delays). Possibly watch server logs if available for command injection signs.",
            "If the file is accessible by download, check the stored name. Did it keep the weird characters? If yes, the risk is higher that somewhere an unsanitized usage could happen (especially if combined into a shell command). If it normalized the name (removed or replaced special chars), that’s safer.",
            "If you know technology in use: e.g., if they use `system()` or similar to handle files, then injection is likely if not careful. Without source, we rely on trying payloads. Another trick: Some systems might execute image metadata (like exif or ICC profiles). Rare but possible – test by including backticks or semi-colons in those fields if you can (advanced, usually not needed unless known image processing step).",
            "Essentially, if an uploaded file name can carry shell metacharacters and you suspect they're used unsafely, you might see either some result of command (if e.g., output of `whoami` got inserted somewhere), or nothing which is inconclusive. This one can be hard to verify definitively black-box, unless the injection is obvious (like it runs and you see effect, e.g., server stops because you ran `; shutdown`). Not recommended to try destructive stuff though!"
          ],
          "tools": [
            {
              "name": "Burp (upload field tampering)",
              "use": "Easily change file name in the request if web UI disallows special chars. Intercept and modify Content-Disposition in multipart form to something malicious like `filename=\"test;`uname`\"`. Then send. See how server responds."
            },
            {
              "name": "Exiftool for payload injection",
              "use": "If file content metadata might be executed by an external tool (like ImageMagick), embed known exploit strings in metadata. For example, older ImageMagick had `label:@|some_command` injection. Most likely patched but if suspect, could test older vector. Use exiftool to set an image label to `\"` or other funky values."
            }
          ],
          "combo_packs": [
            "Often part of **Injection Pack**: beyond SQL and XSS, test command injection in any file operations. The classic being file name injection in system calls. If there's an admin function to scan or decompress the file, also think about injection via those parameters."
          ]
        },
        "grouping": {
          "owasp_wstg": "Falls under OS command injection tests (WSTG-Inj-OSCMD) but specifically triggered by file handling. WSTG likely tells testers to try injecting OS commands wherever user input might go to shell, including filenames.",
          "ptes": "Certainly encompassed by checking all user inputs for injection potential. A PTES tester will not ignore file names – they are user input too and can be entry point to OS commands.",
          "nist_800_115": "Yes, as part of input validation or injection testing. They would encourage testers to supply malicious input to see if any executes.",
          "osstmm": "Same, under injection vulnerabilities and proper input handling. This is a known category of injection.",
          "issaf": "ISSAF likely has test cases for command injection; here we apply it to file inputs. It’s an extension of those test cases."
        },
        "applicability": "If the app handles file names or passes file inputs to system calls. Not all do. If file uploads are just stored and not further processed by OS commands, direct injection risk is lower (though maybe still a risk if used in logs or something). But any image processing, PDF conversion, antivirus scanning could be avenues. So if upload feature exists, test a bit regardless, but especially if any processing is known.",
        "cross_method_example": "In real cases, a web app allowed uploading images but used ImageMagick without patch – attacker created an image with a payload in filename or metadata, resulting in remote code execution on server (this was the ImageTragick exploit). Another scenario: app saves file and calls a shell script like `process.sh <filename>`. If filename has `; evil_command`, that runs. ASVS calls it out because it's happened. It's essentially OS command injection triggered via file. Everyone agrees command injection is critical – this is just a vector for it. So our test cross methodologies is simply: treat any user-controlled part of an OS interaction as dangerous and test it accordingly.",
        "sources": [
          "https://cheatsheetseries.owasp.org/cheatsheets/File_Upload_Cheat_Sheet.html",
          "https://owasp.org/www-community/attacks/OS_Command_Injection"
        ],
        "caveats": [
          "Hard to confirm exploitation without potentially harmful actions. You should not execute a destructive command on a live system. Instead, try benign ones that have output (like `uname` or `id`). But output may not be visible to you unless the app returns something. If nothing visible, maybe try a time-based payload (like `; ping -c 5 127.0.0.1` to see delay). But careful – that could still affect system performance or trigger alerts.",
          "Remember that modern frameworks often handle filenames safely (e.g., .NET and Java will throw if filename has path or illegal chars). So if your attempts are consistently blocked or sanitized, likely the app is safe. But one edge: if the app is on Windows, sometimes allowed special chars differ vs Linux. For instance, `&` might be allowed in NTFS file name but when passed to shell, it's special. That’s a subtle environment issue. Keep that in mind.",
          "If you suspect something but can’t prove, you might mention that best practice is to sanitize file names thoroughly (remove special chars, etc.) and not use unsanitized names in shell calls. But try to avoid false positives – only say it's vulnerable if you have indication. If no evidence, then presumably it's okay."
        ]
      },
      {
        "title": "Remote Code Execution Through FIle Upload (e.g. uploading JSP, ASP etc)",
        "asvs": "12.3.6",
        "simple_desc": "The application must prevent an attacker from uploading a file that the server will execute as code (thus achieving remote code execution, RCE). For instance, uploading a webshell script (.php, .jsp, etc.) into a directory where it can be accessed and run. Proper file type validation (from 12.2.1) and storing files outside web root or without execute permissions (12.4.1) are key defenses. If those fail, an attacker could upload a malicious file and then directly navigate to it to run commands on the server.",
        "references": "- **OWASP Top 10** – Code execution via file upload is a common critical issue (fits under A6 Security Misconfig or A10 older injection category).\n- **OWASP Cheat Sheet** – Storing outside webroot:contentReference[oaicite:149]{index=149} and not allowing script file types prevents this scenario.",
        "how_to_test": {
          "manual": [
            "Try uploading a known server-side script: if the app is on PHP, upload a `.php` file with simple PHP code (`<?php echo 'RCE'; ?>`). If on Java, maybe a `.jsp` or `.jspx` if allowed. If on .NET, `.aspx` or `.ashx`, etc. Many frameworks won't accept these or will rename them, but test it.",
            "If upload appears successful, attempt to access the file via the URL it would have (if you can guess it or if the app gives you a link). For example, if images normally accessible at `uploads/<filename>`, try `uploads/yourfile.php`. See if your 'RCE' string or whatever code output appears, indicating the code executed.",
            "If you can't directly access but suspect it's in web root, perhaps use a minor side effect in code: like a PHP file that writes to a known location or triggers a time delay (not ideal in black box). It's easier when you can access it via URL.",
            "If the application changes the extension (e.g., renames `.php` to `.txt` on store), see if you can bypass that by double extension (`file.php.jpg` might sometimes slip through and still be executed if server misconfig interprets it). Try known bypass tricks for that server type.",
            "If the server is configured to not execute in upload directory, a direct attempt might fail (which is good). But verify by trying – if you just see code as text or a download, then execution didn't happen. If you see execution (like your output or errors), it's vulnerable.",
            "Don't forget other interpreters: uploading a `.shtml` (Server Side Include) could lead to execution if server-side include is enabled. Or uploading a `.config` in some cases might be interpreted by server differently. Focus on primary (like .php on PHP servers)."
          ],
          "tools": [
            {
              "name": "Burp Repeater & Proxy",
              "use": "Use Burp to do the upload and then try to browse the uploaded file through Burp as well. Intercept responses to confirm what's happening. Also, if normal UI doesn't give link, maybe guess path by using any pattern from other uploads (filename might be same as uploaded or changed). Burp can fuzz likely paths."
            },
            {
              "name": "Automated shell upload (commonly in metasploit or others)",
              "use": "Frameworks like Metasploit have file upload exploit modules for known apps, but here we are manually testing. A simpler approach: if environment info known, use a ready webshell payload from a repository (like a one-liner php shell) for the test. Then use the app interface to open it. This essentially is 'exploiting' the vuln if present."
            }
          ],
          "combo_packs": [
            "This is basically the culmination of failing prior controls (type check, storage in web root). So it ties with category checks V12.2.1 and V12.4.1. If those are in place, this likely cannot happen. We test it anyway to confirm no gap. It's the ultimate **Web Shell Upload Pack** – try to upload and run a shell."
          ]
        },
        "grouping": {
          "owasp_wstg": "This scenario is exactly what testers do when they attempt to exploit file upload flaws – upload a script and execute. It's implied in WSTG under unrestricted file upload vulnerabilities.",
          "ptes": "Yes, exploitation phase: if file upload vulnerability exists, the end goal for a pentester is RCE. PTES would have them attempt this once identified type check or storage issues.",
          "nist_800_115": "Yes, part of attack execution. If the system allows, a tester will prove they can run commands by uploading code. It's a direct breach demonstration.",
          "osstmm": "This is a catastrophic failure of controls – remote execution. OSSTMM reporting would classify it as full compromise basically. It's beyond just a vulnerability; it's system breach. But academically, falls under code injection category allowed by file handling flaw.",
          "issaf": "Certainly would try to achieve this in a test. ISSAF in web app testing aims to get shells if possible to show impact."
        },
        "applicability": "If file uploads are allowed and the tech stack has executable file types (most do) and misconfiguration could let them run, test this. If the app only stores files as data and definitely not reachable via web (or on a separate storage server that isn't executable), then it's not applicable. But always verify assumptions – e.g., just because devs say 'uploads not in web root', check if truly inaccessible (maybe direct URL might still fetch it). If no upload feature at all, N/A obviously.",
        "cross_method_example": "Many infamous breaches: someone found they could upload an ASP or PHP webshell and just call it to get a remote shell. ASVS highlights to absolutely prevent this. A real test example: an education platform allowed image uploads for profiles but didn't restrict type or location, an attacker uploaded an `.aspx` webshell and took over the server (common in older apps). After that case, they implemented extension filters and stored files outside web root. It's classic. All methodologies treat the risk of file upload leading to RCE as one of the most critical things to check. If our earlier tests show you can upload `.jsp` and it sits in /uploads accessible, this test confirms by actually running it, which is game over for the app.",
        "sources": [
          "https://cheatsheetseries.owasp.org/cheatsheets/File_Upload_Cheat_Sheet.html"
        ],
        "caveats": [
          "If you successfully upload a shell, be VERY mindful of boundaries. Do not use it to pivot deeper or exfil data unless explicitly allowed. Typically, showing a simple output like running `id` or creating a benign file is enough as proof.",
          "Sometimes execution might not be straightforward: e.g., a .php might not execute if server config doesn't treat that directory as PHP-enabled. Try known scripting types or double extensions. But if none works, don't force it too hard (like messing with server config or something). Just conclude it's likely not executable. Also ensure not a false negative – maybe it's executed but you don't see output due to how script designed. So test a clear symptom (like writing known string to a file or to output).",
          "This test should be last or separate because if it succeeds, you have a shell on server which is beyond vulnerability, it's compromise. That can disrupt the application or raise alarms. In a safe environment, fine. In prod, usually we wouldn't actually drop a heavy shell – maybe a safe test script. Always follow rules of engagement on how far to go. Often showing that a script file is accessible with code content might be enough without executing, but execution proves maximum impact. Gauge carefully."
        ]
      }
    ]
  }
]
